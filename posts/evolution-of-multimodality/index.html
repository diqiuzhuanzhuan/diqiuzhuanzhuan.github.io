<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Evolution of Multimodality | Loong&#39;s Lens</title>
<meta name="keywords" content="multimodality, computer vision">
<meta name="description" content="A series of papers about multimodality.">
<meta name="author" content="Loong">
<link rel="canonical" href="https://diqiuzhuanzhuan.github.io/posts/evolution-of-multimodality/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.755375f2d2f5befc89390aa1da6369388945ce457429a284ad9ed354197440d0.css" integrity="sha256-dVN18tL1vvyJOQqh2mNpOIlFzkV0KaKErZ7TVBl0QNA=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://diqiuzhuanzhuan.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://diqiuzhuanzhuan.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://diqiuzhuanzhuan.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://diqiuzhuanzhuan.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://diqiuzhuanzhuan.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>

<meta property="og:title" content="Evolution of Multimodality" />
<meta property="og:description" content="A series of papers about multimodality." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://diqiuzhuanzhuan.github.io/posts/evolution-of-multimodality/" /><meta property="og:image" content="https://diqiuzhuanzhuan.github.io/images/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-04-22T00:00:00+00:00" />


<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://diqiuzhuanzhuan.github.io/images/papermod-cover.png"/>

<meta name="twitter:title" content="Evolution of Multimodality"/>
<meta name="twitter:description" content="A series of papers about multimodality."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://diqiuzhuanzhuan.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Evolution of Multimodality",
      "item": "https://diqiuzhuanzhuan.github.io/posts/evolution-of-multimodality/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Evolution of Multimodality",
  "name": "Evolution of Multimodality",
  "description": "A series of papers about multimodality.",
  "keywords": [
    "multimodality", "computer vision"
  ],
  "articleBody": "With the swift development of deep neural networks, a multitude of models handling diverse information modalities like text, speech, images, and videos have proliferated. Among AI researchers, it’s widely acknowledged that multimodality is the future of AI. Let’s explore the advancements in multimodality in recent years.\nTexts \u0026 Images CLIP CLIP (radford et al., 20211) thinks learning directly from raw text about images is promising alternative which leverage much a boarder source of supervision. Based on the consideration of computation budget and performance, the authors choose constrastive representation learning over directly predicting objectives. To train this model, we need a text encoder and an image encoder to get text and image representations, and then maximize the consine similarity of them.\nFigure 1. Illustrate how to train and inference.(Image source: Learning Transferable Visual Models From Natural Language Supervision)\nPseudo code for the core of an implementation of CLIP (radford et al., 20211) is here:\nimport numpy as np # image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # joint multimodal embedding [n, d_e] I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # scaled pairwise cosine similarities [n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # symmetric loss function labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2 CLIP (radford et al., 20211) offers significant benifits for that task has relative little data given its zero-shot capability. The study underscores the substantial potential of pre-training techniques for multimodal applications. Therefore, shortly thereafter, numerous applications based on CLIP emerged.\nALIGN ALIGN (jia et al., 20212) leverages a noisy data of over one bilion image alt-text pairs to train a model, which only has a simple dual-encoder architecture, to align visual and language representation of image and text pairs use a contrastive loss.\nFigure 2. A summary of ALIGN methods, which doesn’t need curated data. (Images source: Learning Transferable Visual Models From Natural Language Supervision)\nThe authors only apply minimal frequency-based filtering on image and text, such as filtering alt-texts that are shared by more than 10 images, and filtering irrelevant content (e.g., “1980x1080”, “alt_img”), etc. While pretraining, the authors consturct two losses: one for image-to-text classification, other one for text-to-image classfication $$L_{i2t} = -\\frac{1}{N} \\sum^N_i \\log \\frac{\\exp(x_i^T y_i/\\sigma)}{\\sum_{j=1}^N \\exp(x_i^T y_j/\\sigma)}$$ $$L_{t2i} = -\\frac{1}{N} \\sum^N_i \\log \\frac{\\exp(y_i^T x_i/\\sigma)}{\\sum_{j=1}^N \\exp(y_i^T x_j/\\sigma)}$$ where $x_i$ and $y_j$ are the normalized of image in the $i$-th pair and that of text in the $j$-th pair, respectively. $N$ is the batch size, and $\\sigma$ is the temperature to scale the logits. It is worth noting that the temperature parameter $\\sigma$ is not set manually but learned jointly together with other parameters.\nViLT ViLT (kim et al., 20213) is a simple architecture for a vision-and-language model as it commissions the transformer module to handle vision features in place of a seperate deep vision embedder. This architecture concentrates most of the computation on modality interaction other than feature extraction, thereby achieves competent performance on vision-and-language tasks without using region features or deep convolutional visual encoder.\nFigure 3. Four cagegories of vision-and-language model. CLIP belongs to (b), ViLT belongs to (d). (Image source: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision)\nBuilding on pre-training objectives, the authors use image text matching (ITM) and masked language modeling (MLM). ViLT is competent to competitors which are heavily equipped with convolutional visual embedding networks (e.g., Faster R-CNN and ResNets). Hence, the authors conclude that future work on VLP should focus more on the modality interactions inside the transformer module rather than engaging in an arms race that merely powers up unimodal embedders. Figure 4. ViLT model architecture. (Image source: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision)\nALBEF Most of previous methods choose to employ a transformer-based multimodal encoder to jointly model visual and language tokens. because the visual tokens and text tokens are unaligned, it is challenging for multimodal encoder to learn a optimal interaction. ALBEF (li et al., 20214) introduce a contrastive loss to align the text and image representation before modality fussion. To improve data efficiency from raw noisy data, the study proposes momentum distillation, a self-training method which learns the pseudo targets produced by the momentum model. Figure 5. Illustration of ALBEF. It consists of three encoders: image encoder, text encoder, and a multimodal encoder. (Image source: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation)\nThe pretrain process is relative complex, which includes three objectives: image-text contrastive learning (ITC) on the unimodal encoders, masked language modeling (MLM) learning and image-text matching (ITM) on the multimodal encoder. specifically, the authors improve ITM with online contrastive hard negative mining.\nImage-Text Contrastive learning (ITC): $$p_{m}^{i2t}(I) = \\frac{\\exp(s(I; T_{m})/\\tau)}{\\sum_{m=1}^{M} \\exp(s(I; T_{m})/\\tau)},p_{m}^{t2i}(I) = \\frac{\\exp(s(T; I_{m})/\\tau)}{\\sum_{m=1}^{M} \\exp(s(T; I_{m})/\\tau)}$$\n$$\\mathcal{L}_{\\text{itc}} = \\frac{1}{2} \\mathbb{E}_{(I; T) \\sim{D} } [ \\text{H}(y^{i2t} (I), p^{i2t} (I)) + \\text{H} (y^{t2i}(T), p^{t2i} (T) )]$$\nHere, $s(I;T)$ denotes the similarity of image $I$ and text $T$, $\\tau$ is a learnable temperature parameter, $y^{i2t}$ and $y^{t2i}$ denote the one-hot ground truth similarity, and $\\text{H}$ is the cross ent\tropy of two objects. The image-text contrastive loss is $\\mathcal{L}_{\\text{itc}}$.\nMasked Language modeling (MLM):\nMLM task utilizes both the image and the text to predict the masked words. The masking strategy is the same as BERT (devlin et al., 20185). MLM minimazes a cross-entropy loss:\n$$\\mathcal{L}_{mlm} = \\mathbb{E}_{(I, \\widehat{T}) \\sim D} H( y^{msk}, p^{msk} (I, \\widehat{T}))$$\nwhere $\\widehat{T}$ denotes a masked text, $p^{msk}(I, \\widehat{T})$ denotes the model’s probability for a masked token, $y^{msk}$ is a one-hot vocabulary distribution in which the group truth token’s value is 1.\nImage-Text Matching (ITM):\nITM predicts if a pair of text and image is matched or not matched. The CLS token of multimodal encoder is used the joint representation of the image-text pair. The ITM loss is:\n$$\\mathcal{L}_{itm} = \\mathbb{E}_{(I, T) \\sim D} H( y^{itm}, p^{itm} (I, T))$$\nwhere $y^{itm}$ is a 2-dimensional one-hot vector representing the ground truth label.\nFinally, the full objective loss is:\n$$\\mathcal{L} = \\mathcal{L}_{itc} + \\mathcal{L}_{mlm} + \\mathcal{L}_{itm}$$\nMomentum Distillation:\nAs one-hot labels for ITC and MLM penalize all negative predictions regardless of their correctness, to address this, the authors propose to learn from pseudo-targets generated by the momentum model. The momentum model is a continuously evolving teacher which consists of exponential-moving-average (EMA) of the unimodal and multimodal encoders. For ITC task, the momentum loss is:\n$$\\mathcal{L}_{itc}^{mod} = (1 - \\alpha) \\mathcal{L}_{itc} + \\frac{\\alpha}{2} \\mathbb{E}_{(I, T) \\sim D}[KL(q^{i2t}(I)||p^{i2t}(I)) + KL(q^{t2i}(T) || p^{t2i}(T))]$$\nwhere $q^{i2t}$ and $q^{t2i}$ are the pseudo targets generated by the momentum model.\nSimilarly, the MLM momentum loss is:\n$$\\mathcal{L}_{mlm}^{mod} = (1 - \\alpha) \\mathcal{L}_{mlm} + \\alpha\\mathbb{E}_{(I, \\widehat{T}) \\sim D}KL(q^{msk}(I, \\widehat{T})||p^{msk}(I, \\widehat{T}))$$ ALBEF propels the multimodality model to a new height, leading to the development of numerous related studies.\nVLMO VLMO(Bao et al., 20226) presents a unified Vision-Language Pretrained Model (VLMO) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. For the sake of encoding various modalities (images, text, and image-text pairs) within single Transformer block, VLMO introduces Mixture-of-Modality-Experts (MoME). V-FFN is designed for image-only data, and L-FFN is for text-only data. However, during training with iamge-text pair data, all modules (V-FFN, L-FFN and VL-FFN) are utilized. Figure 6. Overview of VLMO pre-training. (Image source: VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts)\nAnother noteworthy case is that self-attention module is frozen during training with only text data. Figure 7. Mixture of multi-experts. (Image source: VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts)\nBLIP Most existing frameworks of vision language model (VLP) only excel in either understanding-based tasks or generation-based tasks. BLIP (li et al., 20227) presents a new framework to transfer flexible to both vision-language understanding and generation tasks. Unlike previous works, BLIP argues the noisy web texts are suboptimal for vision-language learning and address this problem by proposing a novel method.\nBLIP introduces two important contributions: a new model architecture named multimodal mixture of encoder-decoder (MED), and a new data bootstrapping method captioning and filtering (CapFilt) which aims to learn from noisy image-text pairs.\nFigure 8. Pre-training model architecture and objectives of BLIP (same parameters have the same color. ITC task is trained without cross-attention. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\nFigure 9. BLIP learning framworkd, include model workflow and data workflow. The bootstrapped data will be used to pre-train the model. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\nFigure 10. Examples of captions generated by BLIP, green texts is accepted by filter, and red texts is rejected by filter. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\nMAE Vary from past approaches, MAE(he et al., 20228) proposes an architecture that involves masking random patches of an input image and reconstructing the missing pixels. Masking a high proportion of input images, such as 75%, yields a nontrivial and meaningful self-supervised task. In MAE, encoder and decoder are not in symmetry. The decoder is designed to predict the pixel value of the masked patch. The authors also study a variant whose reconstruction target is the normalized pixel values of each masked patch, and find that improves the representation quality.\nFigure 11. The architecture consist of an encoder and decoder. (Image source: Masked Autoencoders Are Scalable Vision Learners)\nTokens vs Pixels: the studiers compare tokens and pixels as the reconstruction terms of decoder, and the experiments show that tokenization is not necessary for MAE.\nFigure 12. Tokens vs Pixels. Tokens don’t bring any benifits. (Image source: Masked Autoencoders Are Scalable Vision Learners)\nMonkey Monkey(li et al., 20239) focuses on enhancing large multimodal models (LMMs) for high-resolution input and detailed scene understanding. Compared to the approach of directly interpolating the ViT to increase input resolution, Monkey utilizes a novel module that divides high-resolution images into smaller patches by a sliding window method. Each patch is processed independently by a static visual encoder, enhanced with LoRA adjustments and a trainable visual resampler.\nFigure 11. Each patch is processed by independently by a static visual encoder, enhanced with LoRA adjustments and a trainable visual resampler。 All patches are processed through the shared static Vit encoder, such as Vit-BigG with 2b parameters. (Image source: Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models)\nMini-Gemini References Radford et al., Learning Transferable Visual Models From Natural Language Supervision, CVPR 2021 ↩︎ ↩︎ ↩︎\nJia et al., Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision, ICML 2021 ↩︎\nKim et al., ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision, ICML 2021 ↩︎\nLi et al., Align before Fuse: Vision and Language Representation Learning with Momentum Distillation, CVPR 2021 ↩︎\nDevlin et al., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL 2018 ↩︎\nBao et al., VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts, CVPR 2022 ↩︎\nLi et al., BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, CVPR 2022 ↩︎\nMasked Autoencoders Are Scalable Vision Learners, CVPR 2022. ↩︎\nLi et al., Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models, CVPR 2023 ↩︎\n",
  "wordCount" : "1890",
  "inLanguage": "en",
  "datePublished": "2023-04-22T00:00:00Z",
  "dateModified": "2023-04-22T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Loong"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://diqiuzhuanzhuan.github.io/posts/evolution-of-multimodality/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Loong's Lens",
    "logo": {
      "@type": "ImageObject",
      "url": "https://diqiuzhuanzhuan.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://diqiuzhuanzhuan.github.io/" accesskey="h" title="Loong&#39;s Lens (Alt + H)">Loong&#39;s Lens</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://diqiuzhuanzhuan.github.io/posts" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://diqiuzhuanzhuan.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://diqiuzhuanzhuan.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://diqiuzhuanzhuan.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://diqiuzhuanzhuan.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://diqiuzhuanzhuan.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Evolution of Multimodality
    </h1>
    <div class="post-description">
      A series of papers about multimodality.
    </div>
    <div class="post-meta"><span title='2023-04-22 00:00:00 +0000 UTC'>April 22, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Loong

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#texts--images" aria-label="Texts &amp; Images">Texts &amp; Images</a><ul>
                        
                <li>
                    <a href="#clip" aria-label="CLIP">CLIP</a></li>
                <li>
                    <a href="#align" aria-label="ALIGN">ALIGN</a></li>
                <li>
                    <a href="#vilt" aria-label="ViLT">ViLT</a></li>
                <li>
                    <a href="#albef" aria-label="ALBEF">ALBEF</a></li>
                <li>
                    <a href="#vlmo" aria-label="VLMO">VLMO</a></li>
                <li>
                    <a href="#blip" aria-label="BLIP">BLIP</a></li>
                <li>
                    <a href="#mae" aria-label="MAE">MAE</a></li>
                <li>
                    <a href="#monkey" aria-label="Monkey">Monkey</a></li>
                <li>
                    <a href="#mini-gemini" aria-label="Mini-Gemini">Mini-Gemini</a></li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>With the swift development of deep neural networks, a multitude of models handling diverse information modalities like text, speech, images, and videos have proliferated. Among AI researchers, it&rsquo;s widely acknowledged that multimodality is the future of AI. Let&rsquo;s explore the advancements in multimodality in recent years.</p>
<h2 id="texts--images">Texts &amp; Images<a hidden class="anchor" aria-hidden="true" href="#texts--images">#</a></h2>
<h3 id="clip">CLIP<a hidden class="anchor" aria-hidden="true" href="#clip">#</a></h3>
<p>CLIP (<a href="https://arxiv.org/abs/2103.00020">radford et al., 2021</a><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>) thinks learning directly from raw text about images is promising alternative which leverage much a boarder source of supervision. Based on the consideration of computation budget and performance, the authors choose constrastive representation learning over directly predicting objectives. To train this model, we need a text encoder and an image encoder to get text  and image representations, and then maximize the consine similarity of them.</p>
<figure>
    <img loading="lazy" src="images/clip_illustrate.png"
         alt="Figure 1. Illustrate how to train and inference."/> <figcaption>
            <p>Figure 1. Illustrate how to train and inference.(Image source: <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a>)</p>
        </figcaption>
</figure>

<p>Pseudo code for the core of an implementation of CLIP  (<a href="https://arxiv.org/abs/2103.00020">radford et al., 2021</a><sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>)  is here:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="c1"># image_encoder - ResNet or Vision Transformer</span>
</span></span><span class="line"><span class="cl"><span class="c1"># text_encoder - CBOW or Text Transformer</span>
</span></span><span class="line"><span class="cl"><span class="c1"># I[n, h, w, c] - minibatch of aligned images</span>
</span></span><span class="line"><span class="cl"><span class="c1"># T[n, l] - minibatch of aligned texts</span>
</span></span><span class="line"><span class="cl"><span class="c1"># W_i[d_i, d_e] - learned proj of image to embed</span>
</span></span><span class="line"><span class="cl"><span class="c1"># W_t[d_t, d_e] - learned proj of text to embed</span>
</span></span><span class="line"><span class="cl"><span class="c1"># t - learned temperature parameter</span>
</span></span><span class="line"><span class="cl"><span class="c1"># extract feature representations of each modality</span>
</span></span><span class="line"><span class="cl"><span class="n">I_f</span> <span class="o">=</span> <span class="n">image_encoder</span><span class="p">(</span><span class="n">I</span><span class="p">)</span> <span class="c1">#[n, d_i]</span>
</span></span><span class="line"><span class="cl"><span class="n">T_f</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="c1">#[n, d_t]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># joint multimodal embedding [n, d_e]</span>
</span></span><span class="line"><span class="cl"><span class="n">I_e</span> <span class="o">=</span> <span class="n">l2_normalize</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">I_f</span><span class="p">,</span> <span class="n">W_i</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">T_e</span> <span class="o">=</span> <span class="n">l2_normalize</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">T_f</span><span class="p">,</span> <span class="n">W_t</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># scaled pairwise cosine similarities [n, n]</span>
</span></span><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">I_e</span><span class="p">,</span> <span class="n">T_e</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># symmetric loss function</span>
</span></span><span class="line"><span class="cl"><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_i</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_t</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_i</span> <span class="o">+</span> <span class="n">loss_t</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
</span></span></code></pre></div><p>CLIP (<a href="https://arxiv.org/abs/2103.00020">radford et al., 2021</a><sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>)  offers significant benifits for that task has relative little data given its zero-shot capability. The study underscores the substantial potential of pre-training techniques for multimodal applications. Therefore, shortly thereafter, numerous applications based on CLIP emerged.</p>
<h3 id="align">ALIGN<a hidden class="anchor" aria-hidden="true" href="#align">#</a></h3>
<p>ALIGN (<a href="https://arxiv.org/abs/2102.05918">jia et al., 2021</a><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>) leverages a noisy data of over one bilion image alt-text pairs to train a model, which only has  a simple dual-encoder architecture, to align visual and language representation of image and text pairs use a contrastive loss.</p>
<p><figure>
    <img loading="lazy" src="images/ALIGN_exp.png"
         alt="Figure 2. A summary of ALIGN methods, which doesn&amp;rsquo;t need curated data. (Images source: Learning Transferable Visual Models From Natural Language Supervision)"/> <figcaption>
            <p>Figure 2. A summary of ALIGN methods, which doesn&rsquo;t need curated data. (Images source: <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a>)</p>
        </figcaption>
</figure>

The authors only apply minimal frequency-based filtering on image and text, such as filtering alt-texts that are shared by more than 10 images, and filtering irrelevant content (e.g., &ldquo;1980x1080&rdquo;, &ldquo;alt_img&rdquo;), etc.
While pretraining, the authors consturct two losses: one for image-to-text classification, other one for text-to-image classfication
$$L_{i2t} = -\frac{1}{N} \sum^N_i \log \frac{\exp(x_i^T y_i/\sigma)}{\sum_{j=1}^N \exp(x_i^T y_j/\sigma)}$$
$$L_{t2i} = -\frac{1}{N} \sum^N_i \log \frac{\exp(y_i^T x_i/\sigma)}{\sum_{j=1}^N \exp(y_i^T x_j/\sigma)}$$
where $x_i$ and $y_j$ are the normalized of image in the $i$-th pair and that of text in the $j$-th pair, respectively. $N$ is the batch size, and $\sigma$ is the temperature to scale the logits. It is worth noting that the temperature parameter $\sigma$ is not set manually but learned jointly together with other parameters.</p>
<h3 id="vilt">ViLT<a hidden class="anchor" aria-hidden="true" href="#vilt">#</a></h3>
<p>ViLT (<a href="https://arxiv.org/abs/2102.03334">kim et al., 2021</a><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>) is a simple architecture for a vision-and-language model as it commissions the transformer module to handle vision features in place of a seperate deep vision embedder. This architecture concentrates most of the computation on modality interaction other than feature extraction, thereby achieves competent performance on vision-and-language tasks without using region features or deep convolutional visual encoder.</p>
<figure>
    <img loading="lazy" src="images/ViLT.png"
         alt="Figure 3. Four cagegories of vision-and-language model. CLIP belongs to (b), ViLT belongs to (d). (Image source: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision)"/> <figcaption>
            <p>Figure 3. Four cagegories of vision-and-language model. CLIP belongs to (b), ViLT belongs to (d). (Image source: <a href="https://arxiv.org/abs/2102.03334">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</a>)</p>
        </figcaption>
</figure>

<p>Building on pre-training objectives, the authors use image text matching (ITM) and masked language modeling (MLM). ViLT is competent to competitors which are heavily equipped with convolutional visual embedding networks (e.g., Faster R-CNN and ResNets). Hence, the authors conclude that  future work on VLP should focus more on the modality interactions inside the transformer module rather than engaging in an arms race that merely powers up unimodal embedders.
<figure>
     <img loading="lazy" src="images/ViLT-model-view.png"
          alt="Figure 4. ViLT model architecture. (Image source: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision)"/> <figcaption>
             <p>Figure 4. ViLT model architecture. (Image source: <a href="https://arxiv.org/abs/2102.03334">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</a>)</p>
         </figcaption>
 </figure>
</p>
<h3 id="albef">ALBEF<a hidden class="anchor" aria-hidden="true" href="#albef">#</a></h3>
<p>Most of previous methods choose to employ a transformer-based multimodal encoder to jointly model visual and language tokens. because the visual tokens and text tokens are unaligned, it is challenging for multimodal encoder to learn a optimal interaction. ALBEF (<a href="https://arxiv.org/abs/2107.07651">li et al., 2021</a><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>) introduce a contrastive loss to align the text and image representation before modality fussion. To improve data efficiency from raw noisy data, the study proposes momentum distillation, a self-training method which learns the pseudo targets produced by the momentum model.
<figure>
    <img loading="lazy" src="images/ALBEF-exp.png"
         alt="Figure 5. Illustration of ALBEF. It consists of three encoders: image encoder, text encoder, and a multimodal encoder. (Image source: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation)"/> <figcaption>
            <p>Figure 5. Illustration of ALBEF. It consists of three encoders: image encoder, text encoder, and a multimodal encoder. (Image source: <a href="https://arxiv.org/abs/2107.07651">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a>)</p>
        </figcaption>
</figure>
</p>
<p>The pretrain process is relative complex, which includes three objectives: image-text contrastive learning (ITC) on the unimodal encoders, masked language modeling (MLM) learning and image-text matching (ITM) on the multimodal encoder. specifically, the authors improve ITM with online contrastive hard negative mining.</p>
<p><strong>Image-Text Contrastive learning (ITC)</strong>:
$$p_{m}^{i2t}(I) = \frac{\exp(s(I; T_{m})/\tau)}{\sum_{m=1}^{M} \exp(s(I; T_{m})/\tau)},p_{m}^{t2i}(I) = \frac{\exp(s(T; I_{m})/\tau)}{\sum_{m=1}^{M} \exp(s(T; I_{m})/\tau)}$$</p>
<p>$$\mathcal{L}_{\text{itc}} = \frac{1}{2} \mathbb{E}_{(I; T) \sim{D} } [ \text{H}(y^{i2t} (I), p^{i2t} (I)) + \text{H} (y^{t2i}(T), p^{t2i} (T) )]$$</p>
<p>Here, $s(I;T)$ denotes the similarity of image $I$ and text $T$, $\tau$ is a learnable temperature parameter, $y^{i2t}$ and $y^{t2i}$ denote the one-hot ground truth similarity, and $\text{H}$ is the cross ent	ropy of two objects. The image-text contrastive loss is $\mathcal{L}_{\text{itc}}$.</p>
<p><strong>Masked Language modeling (MLM)</strong>:</p>
<p>MLM task utilizes both the image and the text to predict the masked words. The masking strategy is the same as BERT (<a href="">devlin et al., 2018</a><sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>). MLM minimazes a cross-entropy loss:</p>
<p>$$\mathcal{L}_{mlm} = \mathbb{E}_{(I, \widehat{T}) \sim D} H( y^{msk}, p^{msk} (I, \widehat{T}))$$</p>
<p>where $\widehat{T}$ denotes a masked text, $p^{msk}(I, \widehat{T})$ denotes the model&rsquo;s probability for a masked token, $y^{msk}$ is a one-hot vocabulary distribution in which the group truth token&rsquo;s value is 1.</p>
<p><strong>Image-Text Matching (ITM)</strong>:</p>
<p>ITM predicts if a pair of text and image is matched or not matched. The CLS token of multimodal encoder is used the joint representation of the image-text pair. The ITM loss is:</p>
<p>$$\mathcal{L}_{itm} = \mathbb{E}_{(I, T) \sim D} H( y^{itm}, p^{itm} (I, T))$$</p>
<p>where $y^{itm}$ is a 2-dimensional one-hot vector representing the ground truth label.</p>
<p>Finally, the full objective loss is:</p>
<p>$$\mathcal{L} = \mathcal{L}_{itc} + \mathcal{L}_{mlm} + \mathcal{L}_{itm}$$</p>
<p><strong>Momentum Distillation</strong>:</p>
<p>As one-hot labels for ITC and MLM penalize all negative predictions regardless of their correctness, to address this, the authors propose to learn from pseudo-targets generated by the momentum model. The momentum model is a continuously evolving teacher which consists of exponential-moving-average (<a href="https://en.wikipedia.org/wiki/Exponential_smoothing">EMA</a>) of the unimodal and multimodal encoders.
For ITC task, the momentum loss is:</p>
<p>$$\mathcal{L}_{itc}^{mod} = (1 - \alpha) \mathcal{L}_{itc} + \frac{\alpha}{2} \mathbb{E}_{(I, T) \sim D}[KL(q^{i2t}(I)||p^{i2t}(I)) + KL(q^{t2i}(T) || p^{t2i}(T))]$$</p>
<p>where $q^{i2t}$ and $q^{t2i}$ are the pseudo targets generated by the momentum model.</p>
<p>Similarly, the MLM momentum loss is:</p>
<p>$$\mathcal{L}_{mlm}^{mod} = (1 - \alpha) \mathcal{L}_{mlm} + \alpha\mathbb{E}_{(I, \widehat{T}) \sim D}KL(q^{msk}(I, \widehat{T})||p^{msk}(I, \widehat{T}))$$
ALBEF propels the multimodality model to a new height, leading to the development of numerous related studies.</p>
<h3 id="vlmo">VLMO<a hidden class="anchor" aria-hidden="true" href="#vlmo">#</a></h3>
<p>VLMO(<a href="https://arxiv.org/abs/2111.02358">Bao et al., 2022</a><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>) presents a unified Vision-Language Pretrained Model (VLMO) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. For the sake of encoding various modalities (images, text, and image-text pairs) within single Transformer block, VLMO introduces Mixture-of-Modality-Experts (MoME). V-FFN is designed for image-only data, and L-FFN is for text-only data. However, during training with iamge-text pair data, all modules (V-FFN, L-FFN and VL-FFN) are utilized.
<figure>
    <img loading="lazy" src="images/VLMO-exp.png"
         alt="Figure 6. Overview of VLMO pre-training. (Image source: VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts)"/> <figcaption>
            <p>Figure 6. Overview of VLMO pre-training. (Image source: <a href="https://arxiv.org/abs/2111.02358">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a>)</p>
        </figcaption>
</figure>
</p>
<p>Another noteworthy case is that self-attention module is frozen during training with only text data.
<figure>
    <img loading="lazy" src="images/VLMO-exp2.png"
         alt="Figure 7. Mixture of multi-experts. (Image source: VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts)"/> <figcaption>
            <p>Figure 7. Mixture of multi-experts. (Image source: <a href="https://arxiv.org/abs/2111.02358">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a>)</p>
        </figcaption>
</figure>
</p>
<h3 id="blip">BLIP<a hidden class="anchor" aria-hidden="true" href="#blip">#</a></h3>
<p>Most existing frameworks of vision language model (VLP) only excel in either understanding-based tasks or generation-based tasks. BLIP (<a href="https://arxiv.org/abs/2201.12086">li et al., 2022</a><sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>) presents a new framework to transfer flexible to both vision-language understanding and generation tasks. Unlike previous works, BLIP argues the noisy web texts are suboptimal for vision-language learning and address this problem by proposing a novel method.</p>
<p>BLIP introduces two important contributions: a new model architecture named multimodal mixture of encoder-decoder (MED), and a new data bootstrapping method captioning and filtering (CapFilt) which aims to learn from noisy image-text pairs.</p>
<figure>
    <img loading="lazy" src="images/BLIP-exp2.png"
         alt="Figure 8. Pre-training model architecture and objectives of BLIP (same parameters have the same color. ITC task is trained without cross-attention. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation."/> <figcaption>
            <p>Figure 8. Pre-training model architecture and objectives of BLIP (same parameters have the same color. ITC task is trained without cross-attention. (Image source: <a href="https://arxiv.org/abs/2201.12086">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>.</p>
        </figcaption>
</figure>

<figure>
    <img loading="lazy" src="images/BLIP-exp3.png"
         alt="Figure 9. BLIP learning framworkd, include model workflow and data workflow. The bootstrapped data will be used to pre-train the model. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation."/> <figcaption>
            <p>Figure 9. BLIP learning framworkd, include model workflow and data workflow. The bootstrapped data will be used to pre-train the model. (Image source: <a href="https://arxiv.org/abs/2201.12086">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>.</p>
        </figcaption>
</figure>

<figure>
    <img loading="lazy" src="images/BLIP-exp4.png"
         alt="Figure 10. Examples of captions generated by BLIP, green texts is accepted by filter, and red texts is rejected by filter. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation."/> <figcaption>
            <p>Figure 10. Examples of captions generated by BLIP, green texts is accepted by filter, and red texts is rejected by filter. (Image source: <a href="https://arxiv.org/abs/2201.12086">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>.</p>
        </figcaption>
</figure>

<h3 id="mae">MAE<a hidden class="anchor" aria-hidden="true" href="#mae">#</a></h3>
<p>Vary from past approaches, MAE(<a href="https://arxiv.org/abs/2111.06377">he et al., 2022</a><sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>) proposes an architecture that involves masking random patches of an input image and reconstructing the missing pixels. Masking a high proportion of input images, such as 75%, yields a nontrivial and meaningful self-supervised task.
In MAE, encoder and decoder are not in symmetry. The decoder is designed to predict the pixel value of the masked patch. The authors also study a variant whose reconstruction target is the normalized pixel values of each masked patch, and find that improves the representation quality.</p>
<figure>
    <img loading="lazy" src="images/MAE-EXP1.png"
         alt="Figure 11. The architecture consist of an encoder and decoder. (Image source: Masked Autoencoders Are Scalable Vision Learners)"/> <figcaption>
            <p>Figure 11. The architecture consist of an encoder and decoder. (Image source: <a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a>)</p>
        </figcaption>
</figure>

<p>Tokens vs Pixels: the studiers compare tokens and pixels as the reconstruction terms of decoder, and the experiments show that tokenization is not necessary for MAE.</p>
<figure>
    <img loading="lazy" src="images/MAE-EXP2.png"
         alt="Figure 12. Tokens vs Pixels. Tokens don&amp;rsquo;t bring any benifits. (Image source: Masked Autoencoders Are Scalable Vision Learners)"/> <figcaption>
            <p>Figure 12. Tokens vs Pixels. Tokens don&rsquo;t bring any benifits. (Image source: <a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a>)</p>
        </figcaption>
</figure>

<h3 id="monkey">Monkey<a hidden class="anchor" aria-hidden="true" href="#monkey">#</a></h3>
<p>Monkey(<a href="https://arxiv.org/abs/2311.06607">li et al., 2023</a><sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>) focuses on enhancing large multimodal models (LMMs) for high-resolution input and detailed scene understanding. Compared to the approach of directly interpolating the ViT to increase input resolution, Monkey utilizes a novel module that divides high-resolution images into smaller patches by a sliding window method. Each patch is processed independently by a static visual encoder, enhanced with LoRA adjustments and a trainable visual resampler.</p>
<figure>
    <img loading="lazy" src="images/Monkey-exp1.png"
         alt="Figure 11. Each patch is processed by independently by a static visual encoder, enhanced with LoRA adjustments and a trainable visual resampler。 All patches are processed through the shared static Vit encoder, such as Vit-BigG with 2b parameters. (Image source: Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models)"/> <figcaption>
            <p>Figure 11. Each patch is processed by independently by a static visual encoder, enhanced with LoRA adjustments and a trainable visual resampler。 All patches are processed through the shared static Vit encoder, such as Vit-BigG with 2b parameters. (Image source: <a href="https://arxiv.org/abs/2311.06607">Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models</a>)</p>
        </figcaption>
</figure>

<h3 id="mini-gemini">Mini-Gemini<a hidden class="anchor" aria-hidden="true" href="#mini-gemini">#</a></h3>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Radford et al., <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a>, CVPR 2021&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Jia et al., <a href="https://arxiv.org/abs/2102.05918">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</a>, ICML 2021&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Kim et al., <a href="https://arxiv.org/abs/2102.03334">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</a>, ICML 2021&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Li et al., <a href="https://arxiv.org/abs/2107.07651">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a>, CVPR 2021&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Devlin et al., <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, NAACL 2018&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Bao et al., <a href="https://arxiv.org/abs/2111.02358">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a>, CVPR 2022&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Li et al., <a href="https://arxiv.org/abs/2201.12086">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>, CVPR 2022&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p><a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a>, CVPR 2022.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Li et al., <a href="https://arxiv.org/abs/2311.06607">Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models</a>, CVPR 2023&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://diqiuzhuanzhuan.github.io/tags/multimodality/">multimodality</a></li>
      <li><a href="https://diqiuzhuanzhuan.github.io/tags/computer-vision/">computer vision</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://diqiuzhuanzhuan.github.io/posts/problems-you-may-encounter-while-distributed-training/">
    <span class="title">« Prev</span>
    <br>
    <span>Problems you may encounter during distributed training</span>
  </a>
  <a class="next" href="https://diqiuzhuanzhuan.github.io/posts/position-encoding-in-transformers/position-encoding-in-transformer/">
    <span class="title">Next »</span>
    <br>
    <span>Positional Encoding in Transformer</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Evolution of Multimodality on x"
            href="https://x.com/intent/tweet/?text=Evolution%20of%20Multimodality&amp;url=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2fevolution-of-multimodality%2f&amp;hashtags=multimodality%2ccomputervision">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Evolution of Multimodality on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2fevolution-of-multimodality%2f&amp;title=Evolution%20of%20Multimodality&amp;summary=Evolution%20of%20Multimodality&amp;source=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2fevolution-of-multimodality%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Evolution of Multimodality on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2fevolution-of-multimodality%2f&title=Evolution%20of%20Multimodality">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Evolution of Multimodality on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2fevolution-of-multimodality%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Evolution of Multimodality on whatsapp"
            href="https://api.whatsapp.com/send?text=Evolution%20of%20Multimodality%20-%20https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2fevolution-of-multimodality%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Evolution of Multimodality on telegram"
            href="https://telegram.me/share/url?text=Evolution%20of%20Multimodality&amp;url=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2fevolution-of-multimodality%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Evolution of Multimodality on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Evolution%20of%20Multimodality&u=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2fevolution-of-multimodality%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://diqiuzhuanzhuan.github.io/">Loong&#39;s Lens</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Align LLMs | Loong&#39;s Lens</title>
<meta name="keywords" content="pretraining, continuous pretraining">
<meta name="description" content="A series of papers about Aligning LLMs.">
<meta name="author" content="Loong">
<link rel="canonical" href="https://diqiuzhuanzhuan.github.io/posts/align-llms/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.755375f2d2f5befc89390aa1da6369388945ce457429a284ad9ed354197440d0.css" integrity="sha256-dVN18tL1vvyJOQqh2mNpOIlFzkV0KaKErZ7TVBl0QNA=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://diqiuzhuanzhuan.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://diqiuzhuanzhuan.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://diqiuzhuanzhuan.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://diqiuzhuanzhuan.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://diqiuzhuanzhuan.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>

<meta property="og:title" content="Align LLMs" />
<meta property="og:description" content="A series of papers about Aligning LLMs." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://diqiuzhuanzhuan.github.io/posts/align-llms/" /><meta property="og:image" content="https://diqiuzhuanzhuan.github.io/images/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-23T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-01-23T00:00:00+00:00" />


<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://diqiuzhuanzhuan.github.io/images/papermod-cover.png"/>

<meta name="twitter:title" content="Align LLMs"/>
<meta name="twitter:description" content="A series of papers about Aligning LLMs."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://diqiuzhuanzhuan.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Align LLMs",
      "item": "https://diqiuzhuanzhuan.github.io/posts/align-llms/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Align LLMs",
  "name": "Align LLMs",
  "description": "A series of papers about Aligning LLMs.",
  "keywords": [
    "pretraining", "continuous pretraining"
  ],
  "articleBody": "After pretraining on vast datasets and supervised fine-tuning with diverse instruction sets, Large Language Models (LLMs) have achieved remarkable capabilities in text generation. However, LLMs can generate seemingly reasonable sequences—-free from grammatical errors and redundant words—-they may still generate content that lacks truthfulness or accuracy. Are there any methods to mitigate these shortcomings? Researchers at OpenAI have framed these issues as the challenge of LLM alignment. Currently, one of the most prominent approaches to address these challenges is Reinforcement Learning from Human Feedback (RLHF). To implement RLHF, OpenAI has adopted the Proximal Policy Optimization (PPO) algorithm.\nmulti-turn instruction tuning Most of instruction-following studies and benchmarks overlook the multi-turn instruction following capability of LLMs, which is actually a more common demand in real-world scenarios. So it would therefore never to be too much of an exggeration to say that multi-turn conversation ability is the most significant part of LLMs.\nParrot: enhancing multi-turn instruction following for LLMs Multi-turn example, contextual information is need to be ultilized by LLMs. (Image source: Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models)\nThe most common interaction way between human and LLMs is multi-turn conversation. Parrot1 presents a solution aiming to enhancing multi-turn instruction following for LLMs.\nDataset Collection: the authors proposes training a specialized Parrot-Ask to generate queries using the available real user-ChatGPT logs based on LLaMA, then employ Parrot-Ask to interact with an assistant LLM and thus collect 40K multi-turn instruction tuning data. Training Parrot-Ask Model: Training the mode is the inverse of standard instruction tuning. Compare to common supervised finetuning methods, the Parrot-Ask model is trained to predict query tokens instead of assistant output tokens. Concretely, the authors use LLaMA-13B-Chat and 90K ShareGPT data to train this model. CaPO dataset Collection: The authors sample 10K dataset which involve contextual information and adapt three strageties to generate negative responses, thus collect 10K Context-Aware Preference Optimazation (CaPO) dataset. The process of Parrot.(a) First, train the Parrot-Ask model on real user-ChatGPT logs to learn how real users pose queries, and utilize it to iteratively interact with ChatGPT to collect multi-turn instructionresponse pairs. (b) Then construct negative responses for queries that rely heavily on context for answering with three strategies to simulate three types of error cases. Finally, use the collected data to train the Parrot-Chat model by (c) instruction tuning and (d) context-aware preference optimization.(Image source: Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models)\nhuman preference optimization Make LLMs refuse to answer unknown questions R-Tuning, introduced in (Zhang et al., 20232), aims to equip Large Language Models (LLMs) with the ability to decline answering unknown questions. It leverages the instruction tuning approach, following a two-step process:\nUncertainty Identification: The model is first evaluated on the training data. By inferring the model on the training data once and comparing the prediction and label, the instruction tuning data is split into uncertain data and certain data. Refusal-Aware Data Construction: Uncertainty expressions are appended to the labels of the certain data points. This newly constructed “refusal-aware data” is then used to fine-tune the LLM, enabling it to recognize and decline unknown questions. The workflow of constructing refusal-aware data. (Image source: R-Tuning: Teaching Large Language Models to Refuse Unknown Questions)\nThe purpose of R-Tuning is to alleviate hallucination of LLMs when facing unknown questions. However, it doesn’t take human preference responses into consideration.\ndirect preference optimization DPO (Direct Preference Optimization)3, which evolved from the pair-wise formulation of the reward model introduced in InstructGPT4, simplifies the RLHF (Reinforcement Learning from Human Feedback) process into a one-step optimization. The loss function has been reformulated as follows: $$\\mathcal{L}_{DPO}(\\pi_{\\theta};\\pi_{ref})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}[\\log\\sigma(\\beta\\log\\frac {\\pi_{\\theta}(y_w|x)} {\\pi_{ref}(y_w|x)} - \\beta\\log\\frac {\\pi_{\\theta}(y_l|x)} {\\pi_{ref}(y_l|x)} )]$$ where $y_w$ represents the accepted or preferred data, while $y_l$ represents the rejected or less preferred data. This formulation clearly demonstrates that DPO optimizes the margin between desirable and undesirable changes, effectively enhancing the model’s ability to generate preferred outputs.\nkto Sometimes, preference dataset with a one-pair format is hard to obtain. In such cases, we can use a set of preference data where each sample only has a label of ‘1’ for accept or a label of ‘-1’ for rejectable? KTO[^4] is proposed for this scenario.\nSun et al., Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models, 2024 ↩︎\nZhang et al., R-Tuning: Teaching Large Language Models to Refuse Unknown Questions, NAACL 2024 ↩︎\nRafailov et al., Direct Preference Optimization: Your Language Model is Secretly a Reward Model, 2023 ↩︎\nOuyang et al., Training language models to follow instructions with human feedback, 2022 ↩︎\n",
  "wordCount" : "745",
  "inLanguage": "en",
  "datePublished": "2024-01-23T00:00:00Z",
  "dateModified": "2024-01-23T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Loong"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://diqiuzhuanzhuan.github.io/posts/align-llms/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Loong's Lens",
    "logo": {
      "@type": "ImageObject",
      "url": "https://diqiuzhuanzhuan.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://diqiuzhuanzhuan.github.io/" accesskey="h" title="Loong&#39;s Lens (Alt + H)">Loong&#39;s Lens</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://diqiuzhuanzhuan.github.io/posts" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://diqiuzhuanzhuan.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://diqiuzhuanzhuan.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://diqiuzhuanzhuan.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://diqiuzhuanzhuan.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://diqiuzhuanzhuan.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Align LLMs
    </h1>
    <div class="post-description">
      A series of papers about Aligning LLMs.
    </div>
    <div class="post-meta"><span title='2024-01-23 00:00:00 +0000 UTC'>January 23, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Loong

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#multi-turn-instruction-tuning" aria-label="multi-turn instruction tuning">multi-turn instruction tuning</a><ul>
                        
                <li>
                    <a href="#parrot-enhancing-multi-turn-instruction-following-for-llms" aria-label="Parrot: enhancing multi-turn instruction following for LLMs">Parrot: enhancing multi-turn instruction following for LLMs</a></li></ul>
                </li>
                <li>
                    <a href="#human-preference-optimization" aria-label="human preference optimization">human preference optimization</a><ul>
                        
                <li>
                    <a href="#make-llms-refuse-to-answer-unknown-questions" aria-label="Make LLMs refuse to answer unknown questions">Make LLMs refuse to answer unknown questions</a></li>
                <li>
                    <a href="#direct-preference-optimization" aria-label="direct preference optimization">direct preference optimization</a></li>
                <li>
                    <a href="#kto" aria-label="kto">kto</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>After pretraining on vast datasets and supervised fine-tuning with diverse instruction sets, Large Language Models (LLMs) have achieved remarkable capabilities in text generation. However, LLMs can generate seemingly reasonable sequences&mdash;-free from grammatical errors and redundant words&mdash;-they may still generate content that lacks truthfulness or accuracy.
Are there any methods to mitigate these shortcomings? Researchers at OpenAI have framed these issues as the challenge of LLM alignment. Currently, one of the most prominent approaches to address these challenges is Reinforcement Learning from Human Feedback (RLHF). To implement RLHF, OpenAI has adopted the Proximal Policy Optimization (PPO) algorithm.</p>
<h3 id="multi-turn-instruction-tuning">multi-turn instruction tuning<a hidden class="anchor" aria-hidden="true" href="#multi-turn-instruction-tuning">#</a></h3>
<p>Most of instruction-following studies and benchmarks overlook the multi-turn instruction following capability of LLMs, which is actually a more common demand in real-world scenarios. So it would therefore never to be too much of an exggeration to say that multi-turn conversation ability is the most significant part of LLMs.</p>
<h4 id="parrot-enhancing-multi-turn-instruction-following-for-llms">Parrot: enhancing multi-turn instruction following for LLMs<a hidden class="anchor" aria-hidden="true" href="#parrot-enhancing-multi-turn-instruction-following-for-llms">#</a></h4>
<p><figure>
    <img loading="lazy" src="images/Parrot-exp2.png"
         alt="Multi-turn example, contextual information is need to be ultilized by LLMs. (Image source: Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models)"/> <figcaption>
            <p>Multi-turn example, contextual information is need to be ultilized by LLMs. (Image source: <a href="https://arxiv.org/abs/2310.07301">Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models</a>)</p>
        </figcaption>
</figure>

The most common interaction way between human and LLMs is multi-turn conversation. Parrot<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> presents a solution aiming to enhancing multi-turn instruction following for LLMs.</p>
<ol>
<li>Dataset Collection: the authors proposes training a specialized Parrot-Ask to generate queries using the available real user-ChatGPT logs based on LLaMA, then employ Parrot-Ask to interact with an assistant LLM and thus collect 40K multi-turn instruction tuning data.</li>
<li>Training Parrot-Ask Model: Training the mode is the inverse of standard instruction tuning. Compare to common supervised finetuning methods, the Parrot-Ask model is trained to predict query tokens instead of assistant output tokens. Concretely, the authors use LLaMA-13B-Chat and 90K ShareGPT data to train this model.</li>
<li>CaPO dataset Collection: The authors sample 10K dataset which involve contextual information and adapt three strageties to generate negative responses, thus collect 10K Context-Aware Preference Optimazation (CaPO) dataset.</li>
</ol>
<figure>
    <img loading="lazy" src="images/Parrot-exp1.png"
         alt="The process of Parrot.(a) First, train the Parrot-Ask model on real user-ChatGPT logs to learn how real users pose queries, and utilize it to iteratively interact with ChatGPT to collect multi-turn instructionresponse pairs. (b) Then construct negative responses for queries that rely heavily on context for answering with three strategies to simulate three types of error cases. Finally, use the collected data to train the Parrot-Chat model by (c) instruction tuning and (d) context-aware preference optimization.(Image source: Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models)"/> <figcaption>
            <p>The process of Parrot.(a) First, train the Parrot-Ask model on real user-ChatGPT logs to learn how real users pose queries, and utilize it to iteratively interact with ChatGPT to collect multi-turn instructionresponse pairs. (b) Then construct negative responses for queries that rely heavily on context for answering with three strategies to simulate three types of error cases. Finally, use the collected data to train the Parrot-Chat model by (c) instruction tuning and (d) context-aware preference optimization.(Image source: <a href="https://arxiv.org/abs/2310.07301">Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models</a>)</p>
        </figcaption>
</figure>

<h3 id="human-preference-optimization">human preference optimization<a hidden class="anchor" aria-hidden="true" href="#human-preference-optimization">#</a></h3>
<h4 id="make-llms-refuse-to-answer-unknown-questions">Make LLMs refuse to answer unknown questions<a hidden class="anchor" aria-hidden="true" href="#make-llms-refuse-to-answer-unknown-questions">#</a></h4>
<p><strong>R-Tuning</strong>, introduced in (<a href="https://arxiv.org/abs/2311.09677">Zhang et al., 2023</a><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>), aims to equip Large Language Models (LLMs) with the ability to decline answering unknown questions. It leverages the instruction tuning approach, following a two-step process:</p>
<ol>
<li>Uncertainty Identification: The model is first evaluated on the training data. By inferring the model on the training data once and comparing the prediction and label, the instruction tuning data is split into uncertain data and certain data.</li>
<li>Refusal-Aware Data Construction: Uncertainty expressions are appended to the labels of the certain data points. This newly constructed &ldquo;refusal-aware data&rdquo; is then used to fine-tune the LLM, enabling it to recognize and decline unknown questions.</li>
</ol>
<figure>
     <img loading="lazy" src="images/R-Tuning-exp1.png"
          alt="The workflow of constructing refusal-aware data. (Image source: R-Tuning: Teaching Large Language Models to Refuse Unknown Questions)"/> <figcaption>
             <p>The workflow of constructing refusal-aware data. (Image source: <a href="https://arxiv.org/abs/2311.09677">R-Tuning: Teaching Large Language Models to Refuse Unknown Questions</a>)</p>
         </figcaption>
 </figure>

<p>The purpose of R-Tuning is to alleviate hallucination of LLMs when facing unknown questions. However, it doesn&rsquo;t take human preference responses into consideration.</p>
<h4 id="direct-preference-optimization">direct preference optimization<a hidden class="anchor" aria-hidden="true" href="#direct-preference-optimization">#</a></h4>
<p><strong>DPO</strong> (Direct Preference Optimization)<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, which evolved from the pair-wise formulation of the reward model introduced in <strong>InstructGPT</strong><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, simplifies the RLHF (Reinforcement Learning from Human Feedback) process into a one-step optimization. The loss function has been reformulated as follows:
$$\mathcal{L}_{DPO}(\pi_{\theta};\pi_{ref})=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}[\log\sigma(\beta\log\frac {\pi_{\theta}(y_w|x)} {\pi_{ref}(y_w|x)} - \beta\log\frac {\pi_{\theta}(y_l|x)} {\pi_{ref}(y_l|x)} )]$$
where $y_w$ represents the accepted or preferred data, while $y_l$ represents the rejected or less preferred data.
This formulation clearly demonstrates that DPO optimizes the margin between desirable and undesirable changes, effectively enhancing the model&rsquo;s ability to generate preferred outputs.</p>
<h4 id="kto">kto<a hidden class="anchor" aria-hidden="true" href="#kto">#</a></h4>
<p>Sometimes, preference dataset with a one-pair format is hard to obtain. In such cases, we can use a set of preference data where each sample only has a label of &lsquo;1&rsquo; for accept or a label of &lsquo;-1&rsquo; for rejectable? KTO[^4] is proposed for this scenario.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Sun et al., <a href="https://arxiv.org/abs/2310.07301">Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models</a>, 2024&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Zhang et al., <a href="https://arxiv.org/abs/2311.09677">R-Tuning: Teaching Large Language Models to Refuse Unknown Questions</a>, NAACL 2024&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Rafailov et al., <a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>, 2023&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Ouyang et al., <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a>, 2022&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://diqiuzhuanzhuan.github.io/tags/pretraining/">pretraining</a></li>
      <li><a href="https://diqiuzhuanzhuan.github.io/tags/continuous-pretraining/">continuous pretraining</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://diqiuzhuanzhuan.github.io/posts/retrieval-augoment-generation/">
    <span class="title">« Prev</span>
    <br>
    <span>RAG</span>
  </a>
  <a class="next" href="https://diqiuzhuanzhuan.github.io/posts/audio-llms/">
    <span class="title">Next »</span>
    <br>
    <span>Audio LLMs</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Align LLMs on x"
            href="https://x.com/intent/tweet/?text=Align%20LLMs&amp;url=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2falign-llms%2f&amp;hashtags=pretraining%2ccontinuouspretraining">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Align LLMs on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2falign-llms%2f&amp;title=Align%20LLMs&amp;summary=Align%20LLMs&amp;source=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2falign-llms%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Align LLMs on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2falign-llms%2f&title=Align%20LLMs">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Align LLMs on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2falign-llms%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Align LLMs on whatsapp"
            href="https://api.whatsapp.com/send?text=Align%20LLMs%20-%20https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2falign-llms%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Align LLMs on telegram"
            href="https://telegram.me/share/url?text=Align%20LLMs&amp;url=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2falign-llms%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Align LLMs on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Align%20LLMs&u=https%3a%2f%2fdiqiuzhuanzhuan.github.io%2fposts%2falign-llms%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://diqiuzhuanzhuan.github.io/">Loong&#39;s Lens</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>

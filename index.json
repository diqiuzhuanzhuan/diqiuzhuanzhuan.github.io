[{"content":"","permalink":"https://diqiuzhuanzhuan.github.io/posts/retrieval-augoment-generation/","summary":"","title":"RAG"},{"content":"Hallucination ","permalink":"https://diqiuzhuanzhuan.github.io/posts/align-llms/","summary":"Hallucination ","title":"Align LLMs"},{"content":"Chinese Dataset English Dataset fineweb: 15 trillion tokens of high quality web data. Thanks to the team from huggingface. They filtered and deduplicated all CommonCrawl between 2013 and 2024. Models trained on FineWeb outperform RefinedWeb, C4, DolmaV1.6, The Pile and SlimPajama.\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/data-manager-for-llms/","summary":"Chinese Dataset English Dataset fineweb: 15 trillion tokens of high quality web data. Thanks to the team from huggingface. They filtered and deduplicated all CommonCrawl between 2013 and 2024. Models trained on FineWeb outperform RefinedWeb, C4, DolmaV1.6, The Pile and SlimPajama.","title":"Data for LLMs"},{"content":"Large language models (LLMs) have already demonstrated significant achievements, many startups make a plan to train their own LLMs. However, training a LLM from scratch remains a big challenge, both in terms of machine costs and the difficulty of data collection. Under this background, continuous pretraining based on some open source LLMs is a considerable alternative.\nDetermine your purpose of your continuous pretraining LLM. In common, standard LLMs may not excel in specific domains like financial, law, or trade. And in these areas, the demands for LLMs are stringent. Given this, consistently training our own LLM is an advantageous decision. The three followings are what we\u0026rsquo;ve got to explore : 1) Is domain-adaptive continual pretraining helpful? 2) How can we adopt data selection strategy? 3) Whether the original capabilities are retained?\nData selection Data is the most essential component during continuous pretraining. How do we select and combine various datasets? In important resampling (Xie et al1), the researchers introduce an approach known as Data Selection with Important Resampling (DSIR). The method utilizes raw and target data in an n-gram feature space to estimate important weights.\nFigure 1. For a raw dataset like The Pile, using an estimator to obtain importance weights, and then select data via the importance weights. (Image source: Data Selection for Language Models via Importance Resampling)\nTraining strategy Learning rate setting In general, we tend to use warming up strategy to finetune downstream models. But some researchers (Gupta et al.2) have drawn a series of interesting conclusions on warming up.\nDon\u0026rsquo;t use maximum learning rate initially to avoid an initial large spike in the loss which leads to no consequence later. A smaller learning rate may preserve more performance on the upstream dataset. Continual pretraining with the latest pretrained checkpoint improves performance. Rewarming is not a good option when the downstream dataset is similar to the upstream dataset. For the same dataset, constant learning rate achieves the best performance. Although constant learning rate can give you a good initialization, rewarming will get better while training long enough. catastrophic forgetting What we commonly know about continual pretraining is catastrophic forgetting. Some standard solutions involve little more than mixing common data or retaining some gradient information like EWA.\n(Li et al.3) finds continual pretraining may cause repetition issues.\nFigure 2. After continual training with traditional chinese dataset, the model starts to repeat a sentence. (Image source: Examining Forgetting in Continual Pre-training of Aligned Large Language Models)\nEvaluation tools The lm-eval python package released by EleutherAI which aims to offer a opensource framework in LLM evaluation for AI researchers. To install it, simply run a command:\npip install lm-eval Here is an interesting blog providing a tutorial for beginners.\nXie et al., Data Selection for Language Models via Importance Resampling, NIPS 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGupta et al., Continual Pre-Training of Large Language Models: How to (re)warm your model?, 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi et al., Examining Forgetting in Continual Pre-training of Aligned Large Language Models, 2024\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/continuous-pretraining/","summary":"Large language models (LLMs) have already demonstrated significant achievements, many startups make a plan to train their own LLMs. However, training a LLM from scratch remains a big challenge, both in terms of machine costs and the difficulty of data collection. Under this background, continuous pretraining based on some open source LLMs is a considerable alternative.\nDetermine your purpose of your continuous pretraining LLM. In common, standard LLMs may not excel in specific domains like financial, law, or trade.","title":"Continual Pretraining"},{"content":"GLIDE ","permalink":"https://diqiuzhuanzhuan.github.io/posts/image-generation/","summary":"GLIDE ","title":"Image Generation"},{"content":"Recently, numerous AGI applications catch the eyes of almost all the people on the internet. Here lists some advanced papers elucidate their reasons.\nDiT The authors explore a new class of diffusion models based on the transformer architecture, Diffusion Transformers (DITs)1. Before their work, using a U-Net backbone to generate the target image is prevalent instead of using a transformer architecture. The authors make some experiments with variants of standard transformer blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Figure 1. DIT architecture. The gray area in the diagram is suboptimal structure. (Image source: Scalable Diffusion Models with Transformers)\nIn general, DiTs mainly introduce three components, patchify, DiT blocks, Decoder. All the diffusion process undergoes under the latent space projected by a VAE encoder. DiTs is based on Visual Transformer architecture (ViT) which operates on sequences of patches.\nFigure 2. Given the input of latent space, DiTs patchify it into a sequence. The length of sequence is relevant to patch size p. (Image source: Scalable Diffusion Models with Transformers)\nAs for DiT block, considering that zero-initializing the final batch norm scale factor $\\gamma$ in each block accelerates large-scale training in supervised learning setting, and that zero-initializing the final convolutional layer in each block prior to any residual connection brings benifits in Diffusion U-Net, the authors design adaLN-Zero block, as illustrated in Figure 1. Then a standard linear decoder is applied to decode sequence tokens into many tensors and get predicted noise and covariance. DiTs is the first transformer-based backbone for diffusion models that outperforms prior U-Net models and has a promising future through scaling it to larger models and token counts.\nVDT VDT (lu et al.2) features transformer blocks with modularized temporal and spatial attention modules to utilize the rich spatial-temporal representation inheried in transformers and introduce a unified spatial-temporal mask modeling mechanism. Figure 3. Main components and pipeline in VDT. (Image source: VDT: General-purpose Video Diffusion Transformers via Mask Modeling)\nLatte In this work, the authors present a novel latent diffusion transformers (Latte3), which adopts a video Transformer as the backbone. Latte a pre-trained variational autoencoder to encode input video into features in latent space, where tokens are extracted from encoded features. Then a series of Transformer blocks are applied to encode these tokens. There are inherent disparities between spatial and temporal information and numerous tokens extracted from input videos, hence the authors design four Transformer-based model variants from the perspective of decomposing the spatial and temporal dimensions of input videos.\nFigure 1. Four model variants are designed to capture spatio-temporal information in videos.Each block depicted in light orange represents a Transformer block. The standard Transformer block is employed in (a) and (b). (Image source: Latte: Latent Diffusion Transformer for Video Generation)\nSuppose there is a video clip in the latent space $V_{L} \\in \\mathbb{R}^{F\\times{H}\\times{W}\\times{C}}$, here $F, H, W, C$ represent the number of frames, heights, widths, and channel of video frames in the latent space respectively. Then we translate $V_L$ into a sequence of tokens, denoted as $\\hat{z}\\in \\mathbb{R}^{n_f\\times{n_h}\\times{n_w}\\times{d}}$. For the input of our model, $z = \\hat{z} + p$, where p means the spatio-temporal position embedding. For the spatial Transformer block, the authors reshape $z$ into $z_s \\in \\mathbb{R}^{n_f\\times{t}\\times{d}}$ (here $t = n_h \\times{n_w}$), and then for the temporal Transformer block, the workers reshape $z_s$ into $z_t \\in \\mathbb{R}^{t\\times{n_f}\\times{d}}$ as the input.\nTo embed a video clip, the authors also explore two methods: Uniform frame patch embedding and Compression frame patch embedding. In the first method, $n_f$, $n_h$, $n_w$ correspond to $F$, $\\frac{H}{h}$, and $\\frac{W}{w}$ when non-overlapping image patches are extracted from every video frame. In the other method, $n_f$ is equivalent to $\\frac{F}{s}$ in contrast to non-overlapping uniform patch embedding. In short, \u0026lsquo;Compression\u0026rsquo;, means a few frames are compressed.\nFigure 2. (a) uniform frame patch embedding. (b) compression frame patch embedding. (Image source: Latte: Latent Diffusion Transformer for Video Generation)\nFigure 3. (Image source: Image source: Latte: Latent Diffusion Transformer for Video Generation)\nText2Video-Zero TextVideo-Zero(Khachatryan et al.4) is a totally training-free, does not require massive computation powers or dozens of GPUs.\nPeeples et al., Scalable Diffusion Models with Transformers, CVPR 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLu et al., VDT: General-purpose Video Diffusion Transformers via Mask Modeling, CVPR 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMa et al., Latte: Latent Diffusion Transformer for Video Generation, CVPR 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKhachatryan et al., Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators, 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/video-generation/","summary":"Recently, numerous AGI applications catch the eyes of almost all the people on the internet. Here lists some advanced papers elucidate their reasons.\nDiT The authors explore a new class of diffusion models based on the transformer architecture, Diffusion Transformers (DITs)1. Before their work, using a U-Net backbone to generate the target image is prevalent instead of using a transformer architecture. The authors make some experiments with variants of standard transformer blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens.","title":"Video Generation"},{"content":"Large Language Models (LLMs) have show great promise in various artificial intelligence applications. It is becoming a trend to train a Large Language Model. Nevertheless even for many senior AI engineers, training these complex models remain a significant challenge. Here lists a series of issues you may encounter in the future.\ntorch.distributed.barrier() stuck during training with multi gpus At first, you should try to set the environment variable \u0026lsquo;NCCL_P2P_DISABLE=1\u0026rsquo;. If it works out, the solution is probably to disable ACS of Pcie in BIOS. You may need to reference to the link.\nraise RuntimeError(\u0026ldquo;Ninja is required to load C++ extensions\u0026rdquo;) You need to make sure Ninja compile system has been installed.\nsudo apt install ninja-build subprocess.Called.ProcessError: Command \u0026lsquo;[\u0026lsquo;which\u0026rsquo;, \u0026lsquo;c++\u0026rsquo;]\u0026rsquo; required no-zero exit status 1. This error means each node has to install c++ compiler. So just check if it has been installed on every machine.\nConnection reset by peer in function _create_c10d_store in file rendezvous.py. The default TCP server is set to run on the process denoted as rank0. However, since many processes start at different time, the process on rank0 can\u0026rsquo;t ensure that the server has already started before all of other client processes start. To address this issue, we need to ensure that the server (rank0) starts first.\nAttributeError: \u0026lsquo;LlamaAttention\u0026rsquo; object has no attribute \u0026lsquo;rope_theta\u0026rsquo;. To solve this problem, update transformers to 4.34.0 or above (Link).\nHow to install and update cuda? Reference to this.\nimprove your efficiency of llm resize your vocabulary equivalent to multiple of 8\nWhat should we do if the version of cuda is not match with my torch? The best solution is that install different version of cuda first, and switch on your demand. How to do it? Just run this command and select your desired item.\nupdate-alternatives --config cuda ","permalink":"https://diqiuzhuanzhuan.github.io/posts/problems-you-may-encounter-while-distributed-training/","summary":"Large Language Models (LLMs) have show great promise in various artificial intelligence applications. It is becoming a trend to train a Large Language Model. Nevertheless even for many senior AI engineers, training these complex models remain a significant challenge. Here lists a series of issues you may encounter in the future.\ntorch.distributed.barrier() stuck during training with multi gpus At first, you should try to set the environment variable \u0026lsquo;NCCL_P2P_DISABLE=1\u0026rsquo;. If it works out, the solution is probably to disable ACS of Pcie in BIOS.","title":"Problems you may encounter during distributed training"},{"content":"With the swift development of deep neural networks, a multitude of models handling diverse information modalities like text, speech, images, and videos have proliferated. Among AI researchers, it\u0026rsquo;s widely acknowledged that multimodality is the future of AI. Let\u0026rsquo;s explore the advancements in multimodality in recent years.\nTexts \u0026amp; Images CLIP CLIP (radford et al., 20211) thinks learning directly from raw text about images is promising alternative which leverage much a boarder source of supervision. Based on the consideration of computation budget and performance, the authors choose constrastive representation learning over directly predicting objectives. To train this model, we need a text encoder and an image encoder to get text and image representations, and then maximize the consine similarity of them.\nFigure 1. Illustrate how to train and inference.(Image source: Learning Transferable Visual Models From Natural Language Supervision)\nPseudo code for the core of an implementation of CLIP (radford et al., 20211) is here:\nimport numpy as np # image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # joint multimodal embedding [n, d_e] I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # scaled pairwise cosine similarities [n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # symmetric loss function labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2 CLIP (radford et al., 20211) offers significant benifits for that task has relative little data given its zero-shot capability. The study underscores the substantial potential of pre-training techniques for multimodal applications. Therefore, shortly thereafter, numerous applications based on CLIP emerged.\nALIGN ALIGN (jia et al., 20212) leverages a noisy data of over one bilion image alt-text pairs to train a model, which only has a simple dual-encoder architecture, to align visual and language representation of image and text pairs use a contrastive loss.\nFigure 2. A summary of ALIGN methods, which doesn\u0026rsquo;t need curated data. (Images source: Learning Transferable Visual Models From Natural Language Supervision)\nThe authors only apply minimal frequency-based filtering on image and text, such as filtering alt-texts that are shared by more than 10 images, and filtering irrelevant content (e.g., \u0026ldquo;1980x1080\u0026rdquo;, \u0026ldquo;alt_img\u0026rdquo;), etc. While pretraining, the authors consturct two losses: one for image-to-text classification, other one for text-to-image classfication $$L_{i2t} = -\\frac{1}{N} \\sum^N_i \\log \\frac{\\exp(x_i^T y_i/\\sigma)}{\\sum_{j=1}^N \\exp(x_i^T y_j/\\sigma)}$$ $$L_{t2i} = -\\frac{1}{N} \\sum^N_i \\log \\frac{\\exp(y_i^T x_i/\\sigma)}{\\sum_{j=1}^N \\exp(y_i^T x_j/\\sigma)}$$ where $x_i$ and $y_j$ are the normalized of image in the $i$-th pair and that of text in the $j$-th pair, respectively. $N$ is the batch size, and $\\sigma$ is the temperature to scale the logits. It is worth noting that the temperature parameter $\\sigma$ is not set manually but learned jointly together with other parameters.\nViLT ViLT (kim et al., 20213) is a simple architecture for a vision-and-language model as it commissions the transformer module to handle vision features in place of a seperate deep vision embedder. This architecture concentrates most of the computation on modality interaction other than feature extraction, thereby achieves competent performance on vision-and-language tasks without using region features or deep convolutional visual encoder.\nFigure 3. Four cagegories of vision-and-language model. CLIP belongs to (b), ViLT belongs to (d). (Image source: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision)\nBuilding on pre-training objectives, the authors use image text matching (ITM) and masked language modeling (MLM). ViLT is competent to competitors which are heavily equipped with convolutional visual embedding networks (e.g., Faster R-CNN and ResNets). Hence, the authors conclude that future work on VLP should focus more on the modality interactions inside the transformer module rather than engaging in an arms race that merely powers up unimodal embedders. Figure 4. ViLT model architecture. (Image source: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision)\nALBEF Most of previous methods choose to employ a transformer-based multimodal encoder to jointly model visual and language tokens. because the visual tokens and text tokens are unaligned, it is challenging for multimodal encoder to learn a optimal interaction. ALBEF (li et al., 20214) introduce a contrastive loss to align the text and image representation before modality fussion. To improve data efficiency from raw noisy data, the study proposes momentum distillation, a self-training method which learns the pseudo targets produced by the momentum model. Figure 5. Illustration of ALBEF. It consists of three encoders: image encoder, text encoder, and a multimodal encoder. (Image source: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation)\nThe pretrain process is relative complex, which includes three objectives: image-text contrastive learning (ITC) on the unimodal encoders, masked language modeling (MLM) learning and image-text matching (ITM) on the multimodal encoder. specifically, the authors improve ITM with online contrastive hard negative mining.\nImage-Text Contrastive learning (ITC): $$p_{m}^{i2t}(I) = \\frac{\\exp(s(I; T_{m})/\\tau)}{\\sum_{m=1}^{M} \\exp(s(I; T_{m})/\\tau)},p_{m}^{t2i}(I) = \\frac{\\exp(s(T; I_{m})/\\tau)}{\\sum_{m=1}^{M} \\exp(s(T; I_{m})/\\tau)}$$\n$$\\mathcal{L}_{\\text{itc}} = \\frac{1}{2} \\mathbb{E}_{(I; T) \\sim{D} } [ \\text{H}(y^{i2t} (I), p^{i2t} (I)) + \\text{H} (y^{t2i}(T), p^{t2i} (T) )]$$\nHere, $s(I;T)$ denotes the similarity of image $I$ and text $T$, $\\tau$ is a learnable temperature parameter, $y^{i2t}$ and $y^{t2i}$ denote the one-hot ground truth similarity, and $\\text{H}$ is the cross ent\tropy of two objects. The image-text contrastive loss is $\\mathcal{L}_{\\text{itc}}$.\nMasked Language modeling (MLM):\nMLM task utilizes both the image and the text to predict the masked words. The masking strategy is the same as BERT (devlin et al., 20185). MLM minimazes a cross-entropy loss:\n$$\\mathcal{L}_{mlm} = \\mathbb{E}_{(I, \\widehat{T}) \\sim D} H( y^{msk}, p^{msk} (I, \\widehat{T}))$$\nwhere $\\widehat{T}$ denotes a masked text, $p^{msk}(I, \\widehat{T})$ denotes the model\u0026rsquo;s probability for a masked token, $y^{msk}$ is a one-hot vocabulary distribution in which the group truth token\u0026rsquo;s value is 1.\nImage-Text Matching (ITM):\nITM predicts if a pair of text and image is matched or not matched. The CLS token of multimodal encoder is used the joint representation of the image-text pair. The ITM loss is:\n$$\\mathcal{L}_{itm} = \\mathbb{E}_{(I, T) \\sim D} H( y^{itm}, p^{itm} (I, T))$$\nwhere $y^{itm}$ is a 2-dimensional one-hot vector representing the ground truth label.\nFinally, the full objective loss is:\n$$\\mathcal{L} = \\mathcal{L}_{itc} + \\mathcal{L}_{mlm} + \\mathcal{L}_{itm}$$\nMomentum Distillation:\nAs one-hot labels for ITC and MLM penalize all negative predictions regardless of their correctness, to address this, the authors propose to learn from pseudo-targets generated by the momentum model. The momentum model is a continuously evolving teacher which consists of exponential-moving-average (EMA) of the unimodal and multimodal encoders. For ITC task, the momentum loss is:\n$$\\mathcal{L}_{itc}^{mod} = (1 - \\alpha) \\mathcal{L}_{itc} + \\frac{\\alpha}{2} \\mathbb{E}_{(I, T) \\sim D}[KL(q^{i2t}(I)||p^{i2t}(I)) + KL(q^{t2i}(T) || p^{t2i}(T))]$$\nwhere $q^{i2t}$ and $q^{t2i}$ are the pseudo targets generated by the momentum model.\nSimilarly, the MLM momentum loss is:\n$$\\mathcal{L}_{mlm}^{mod} = (1 - \\alpha) \\mathcal{L}_{mlm} + \\alpha\\mathbb{E}_{(I, \\widehat{T}) \\sim D}KL(q^{msk}(I, \\widehat{T})||p^{msk}(I, \\widehat{T}))$$ ALBEF propels the multimodality model to a new height, leading to the development of numerous related studies.\nVLMO VLMO(Bao et al., 20226) presents a unified Vision-Language Pretrained Model (VLMO) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. For the sake of encoding various modalities (images, text, and image-text pairs) within single Transformer block, VLMO introduces Mixture-of-Modality-Experts (MoME). V-FFN is designed for image-only data, and L-FFN is for text-only data. However, during training with iamge-text pair data, all modules (V-FFN, L-FFN and VL-FFN) are utilized. Figure 6. Overview of VLMO pre-training. (Image source: VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts)\nAnother noteworthy case is that self-attention module is frozen during training with only text data. Figure 7. Mixture of multi-experts. (Image source: VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts)\nBLIP Most existing frameworks of vision language model (VLP) only excel in either understanding-based tasks or generation-based tasks. BLIP (li et al., 20227) presents a new framework to transfer flexible to both vision-language understanding and generation tasks. Unlike previous works, BLIP argues the noisy web texts are suboptimal for vision-language learning and address this problem by proposing a novel method.\nBLIP introduces two important contributions: a new model architecture named multimodal mixture of encoder-decoder (MED), and a new data bootstrapping method captioning and filtering (CapFilt) which aims to learn from noisy image-text pairs.\nFigure 8. Pre-training model architecture and objectives of BLIP (same parameters have the same color. ITC task is trained without cross-attention. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\nFigure 9. BLIP learning framworkd, include model workflow and data workflow. The bootstrapped data will be used to pre-train the model. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\nFigure 10. Examples of captions generated by BLIP, green texts is accepted by filter, and red texts is rejected by filter. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\nMAE Vary from past approaches, MAE(he et al., 20228) proposes an architecture that involves masking random patches of an input image and reconstructing the missing pixels. Masking a high proportion of input images, such as 75%, yields a nontrivial and meaningful self-supervised task. In MAE, encoder and decoder are not in symmetry. The decoder is designed to predict the pixel value of the masked patch. The authors also study a variant whose reconstruction target is the normalized pixel values of each masked patch, and find that improves the representation quality.\nFigure 11. The architecture consist of an encoder and decoder. (Image source: Masked Autoencoders Are Scalable Vision Learners)\nTokens vs Pixels: the studiers compare tokens and pixels as the reconstruction terms of decoder, and the experiments show that tokenization is not necessary for MAE.\nFigure 12. Tokens vs Pixels. Tokens don\u0026rsquo;t bring any benifits. (Image source: Masked Autoencoders Are Scalable Vision Learners)\nMonkey Monkey(li et al., 20239) focuses on enhancing large multimodal models (LMMs) for high-resolution input and detailed scene understanding. Compared to the approach of directly interpolating the ViT to increase input resolution, Monkey utilizes a novel module that divides high-resolution images into smaller patches by a sliding window method. Each patch is processed independently by a static visual encoder, enhanced with LoRA adjustments and a trainable visual resampler.\nFigure 11. Each patch is processed by independently by a static visual encoder, enhanced with LoRA adjustments and a trainable visual resampler。 All patches are processed through the shared static Vit encoder, such as Vit-BigG with 2b parameters. (Image source: Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models)\nMini-Gemini References Radford et al., Learning Transferable Visual Models From Natural Language Supervision, CVPR 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJia et al., Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision, ICML 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKim et al., ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision, ICML 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi et al., Align before Fuse: Vision and Language Representation Learning with Momentum Distillation, CVPR 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDevlin et al., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBao et al., VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts, CVPR 2022\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi et al., BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, CVPR 2022\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMasked Autoencoders Are Scalable Vision Learners, CVPR 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi et al., Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models, CVPR 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/evolution-of-multimodality/","summary":"With the swift development of deep neural networks, a multitude of models handling diverse information modalities like text, speech, images, and videos have proliferated. Among AI researchers, it\u0026rsquo;s widely acknowledged that multimodality is the future of AI. Let\u0026rsquo;s explore the advancements in multimodality in recent years.\nTexts \u0026amp; Images CLIP CLIP (radford et al., 20211) thinks learning directly from raw text about images is promising alternative which leverage much a boarder source of supervision.","title":"Evolution of Multimodality"},{"content":"With the advancement of large language models (LLMs), the significance of the context length they can handle is increasingly apparent. Let\u0026rsquo;s take a look at the evolution of positional encoding over the years to enhance the context processing capability of LLMs.\nVanilla Positional Encoding Why does Transformer need positional encoding? Actually, Transformer contains no recurrence and no convolution. To help the model to ultilize the order of the sequence, Vanilla Transformer (vaswani et al., 20171) introduced the concept of positional encoding and adopted a simple yet effective approach, using sine and cosine functions to generate positional encodings. This method allows the model to effectively capture the positional information of words in the sequence without adding additional parameters.\n$$ PE(pos,2i) = sin(pos/10000^{2i/d_{model}}) $$ $$PE(pos,2i+1) = cos(pos/10000^{2i/d_{model}})$$ where $pos$ is the position and $i$ is the dimension, $d_{model}$ is the word embedding dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2π$ to $10000 · 2π$. The authors chose this function because they hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.\nOf course, the authors also make a comparision with learned positon embedding (gehring et al., 20172), and the two versions all produce nearly identical results. Nevertheless, the sinusoidal version may allow the model to extrapolate to sequence lengths not encountered during training.\nRelative Positional Encoding Other view: Positonal Encoding is Nothing but computational buget Almost all the people think postional encoding is vital for Transformer though, a team from IBM Research, Facebook CIFAR AI, .etc, get a radically different conclusion (kazemnejad et al., 2024)3, that is position encodings are not essential for decoder-only Transformers to generalize to longer sequences.\nReferences Vaswani et al., \u0026ldquo;Attention Is All Your Need\u0026rdquo;, AAAI 2017\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGehring et al., \u0026ldquo;Convolutional Sequence to Sequence Learning\u0026rdquo;, NIPS 2017\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKazemnejad et al., \u0026ldquo;The Impact of Positional Encoding on Length Generalization in Transformers\u0026rdquo;, NIPS 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/position-encoding-in-transformers/position-encoding-in-transformer/","summary":"With the advancement of large language models (LLMs), the significance of the context length they can handle is increasingly apparent. Let\u0026rsquo;s take a look at the evolution of positional encoding over the years to enhance the context processing capability of LLMs.\nVanilla Positional Encoding Why does Transformer need positional encoding? Actually, Transformer contains no recurrence and no convolution. To help the model to ultilize the order of the sequence, Vanilla Transformer (vaswani et al.","title":"Positional Encoding in Transformer"},{"content":"What Is Generative Models? A generative model can be seen as a way to model the conditional probability of the observed $X$ given a target $y$ (e.g., given a target \u0026lsquo;dog\u0026rsquo;, generate a picture of the dog). Once trained, we can easily sample a stance of $X$. While training a generative model is significantly more challenging than a discriminative model (e.g., it is more difficult to generate an image of a dog than to identify a dog in a picture), it offers the ability to create entirely new data.\nLatent Variable Model for the data $x$ we observe, we imagine a latent variable $z$ and model them as a joint distribution $p(x, z)$. Therefore we have this form: $$p(x) = \\int p(x,z)dz$$ Apply bayes\u0026rsquo;s theorem, so: $$p(x) = \\frac{p(x,z)}{p(z|x)}$$ The log-likelihood of $p(x)$ is below: $$\\begin{align*} \\log p(x) \u0026amp;= \\log \\int p(x,z)dz \\\\ \u0026amp;= \\log \\int \\frac{p(x,z)q_{\\phi}(z|x)}{q_{\\phi}(z|x)}dz \\\\ \u0026amp;= \\log \\mathbb{E}_{q_{\\phi}(z|x)}\\frac{p(x,z)}{q_{\\phi}(z|x)} \\\\ \u0026amp;\\geq \\mathbb{E}_{q_{\\phi}(z|x)} \\log \\frac{p(x,z)}{q_{\\phi}(z|x)} \\\\ \\end{align*}$$From above, we derive the term $\\mathbb{E}_{q_{\\phi}(z|x)} \\log \\frac{p(x,z)}{q_{\\phi}(z|x)}$ called Evidence Lower Bound (ELBO), therefore maximizing the ELBO becomes a proxy objective with which to optimize a latent variable model.\nVariable Autoencoders (VAE) The purpose of the variable autoencoders is to maximize ELBO by optimizing for the best $q_{\\phi}(z|x)$ amongst a family of posterior distribution parameters by $\\phi$. $$\\begin{align*} \\mathbb{E}_{q_{\\phi}(z|x)} \\log \\frac{p(x,z)}{q_{\\phi}(z|x)} \u0026amp;= \\mathbb{E}_{q_{\\phi}(z|x)} \\log \\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi}(z|x)} \\\\ \u0026amp;= \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] + \\mathbb{E}_{q_{\\phi}(z|x)} [\\log \\frac{p(z)}{q_{\\phi}(z|x)}] \\\\ \u0026amp;= \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{\\text{KL}}(q_{\\phi}(z|x) \\parallel p(z)) \\end{align*}$$ In conclusion, we obtain a decoder term $\\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]$ and an encoder term $D_{\\text{KL}}(q_{\\phi}(z|x) \\parallel p(z))$. Our objective is to maximize the first term and minimize the second term. The encoder of VAE is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior is typically assumed to follow a standard multivariate Gaussian: $$q_{\\phi}(z|x) = \\mathcal{N}(z;\\mu_{\\phi}(x),\\sigma^{2}(x)\\text{I})$$ $$p(z) = \\mathcal{N}(z;0,\\text{I})$$ And then the objective function can be rewritten through Monte Carlo sampling as below (here $z^{(l)}$ is sampled from $q_{\\phi}(z|x)$ for every $x$ in the dataset: $$\\underset{\\phi, \\theta}{\\mathrm{arg\\ max}}\\ \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{\\text{KL}}(q_{\\phi}(z|x) \\parallel p(z)) \\ \\approx \\underset{\\phi, \\theta}{\\mathrm{arg\\ max}} \\sum_{l=1}^{L}\\log p_{\\theta}(x|z^{(l)})-D_{\\text{KL}}(q_{\\phi}(z|x) \\parallel p(z))$$ there has been still a problem that $z^{(l)}$ is sampled and unable to optimize throuth gradient descent. To solve this issue, reparameterization trick rewrite the random variable as a deterministic function of a noise variable. $$z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon\\ \\ where\\ \\ \\ \\ \\epsilon \\in \\mathcal{N}(\\epsilon; 0,\\text{I})$$ where $\\odot$ represents an element-wise product. Under this reparameterized version of $z$, gradients can be computed by optimizing $\\mu_{\\phi}$ and $\\sigma_\\phi$. After training, new data can be generated while sample a latent variable from $p(z)$ and feed it into the decoder of VAE. Furthermore, when a powerful semantic latent space is learned, latent vector can be edited or controled before being passed into the decode to generate the desired data.\nHierarchical Variational Autoencoders ","permalink":"https://diqiuzhuanzhuan.github.io/posts/know-about-diffusion-models/","summary":"What Is Generative Models? A generative model can be seen as a way to model the conditional probability of the observed $X$ given a target $y$ (e.g., given a target \u0026lsquo;dog\u0026rsquo;, generate a picture of the dog). Once trained, we can easily sample a stance of $X$. While training a generative model is significantly more challenging than a discriminative model (e.g., it is more difficult to generate an image of a dog than to identify a dog in a picture), it offers the ability to create entirely new data.","title":"Know about diffusion models"}]
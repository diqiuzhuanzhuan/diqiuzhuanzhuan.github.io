[{"content":"","permalink":"https://diqiuzhuanzhuan.github.io/posts/llm-deploy/","summary":"","title":"LLM deploy"},{"content":"RankRAG LLMs are not good at reading too many chunked contexts (e.g., top-100) even with the long-context window. RankRAG aims to design an RAG instruction tuning pipeline that uses a single language model to achieve both high-recall context extraction and high-quality content generation. It is the most significant achievement that both context ranking and answer generation are considered in this framework. Figure 1. The pipeline of RankRAG. (Image source: RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs)\nstage I: Supervised Fine-Tuning (SFT) The authors use 128K SFT examples (e.g., OpenAssistant, Dolly, SODA, ELI5, Self-Istruct, Unnatural Instructions) in total, and take the multi-turn conversion format, use the previous turns of conversation between user and assistant as the context, and only compute the loss at the last response from the assistant. stage II: Unified Instruction-Tuning for Ranking and Generation the stage II consists of these following parts: SFT data from Stage-I: need to maintain the capability of following instruction. Context-rich QA data: i) standard QA and reading comprehension dataset: DROP, NarrativeQA, Quoref, ROPES, NewsQA, TAT-QA. ii) conversational QA datasets: HumanAnnotatedConvQA, SyntheticConvQA. Retrieval-augmented QA data: SQuAD, WebQuestions. In these two datasets, not all the retrieved contexts contain the answer, thus they can be thought of as involving \u0026lsquo;hard-negative\u0026rsquo; contexts. Context ranking data: MS MARCO passage ranking dataset. Retrieval-augmented ranking data:SQuAD, WebQuestions. For each example, combine a gold context with the other retrieved contexts using BM25. LLM is trained to explicitly identify all relevant contexts for the question. Finally, all the above data will be cast into a standardized QA form ($x$, $c$, $y$), where $x$ is question, $c$ is the corresponding context, and $y$ is the target output answer. Figure 2. The converting form of the standardlized QA form from question, context and answer. (Image source: RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs)\nRankRAG Inference: Retrieve-Rerank-Generate Pipeline The process of RankRAG can be described as follows: 1) the retriever $\\mathcal{R}$ retrieves top-$N$ contexts from the knowledge base. 2) the RankRAG model calculates the relevant score between the quetion and retrieved $N$ contexts and only retains the top-$k$ contexts. 3) The top-$k$ contexts, along with the question, are integrated into a long prompt and fed into the RankRAG model to generate the final answer. ","permalink":"https://diqiuzhuanzhuan.github.io/posts/retrieval-augoment-generation/","summary":"RankRAG LLMs are not good at reading too many chunked contexts (e.g., top-100) even with the long-context window. RankRAG aims to design an RAG instruction tuning pipeline that uses a single language model to achieve both high-recall context extraction and high-quality content generation. It is the most significant achievement that both context ranking and answer generation are considered in this framework. Figure 1. The pipeline of RankRAG. (Image source: RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs)","title":"RAG"},{"content":"After pretraining on vast datasets and supervised fine-tuning with diverse instruction sets, Large Language Models (LLMs) have achieved remarkable capabilities in text generation. However, LLMs can generate seemingly reasonable sequences\u0026mdash;-free from grammatical errors and redundant words\u0026mdash;-they may still generate content that lacks truthfulness or accuracy. Are there any methods to mitigate these shortcomings? Researchers at OpenAI have framed these issues as the challenge of LLM alignment. Currently, one of the most prominent approaches to address these challenges is Reinforcement Learning from Human Feedback (RLHF). To implement RLHF, OpenAI has adopted the Proximal Policy Optimization (PPO) algorithm.\nmulti-turn instruction tuning Most of instruction-following studies and benchmarks overlook the multi-turn instruction following capability of LLMs, which is actually a more common demand in real-world scenarios. So it would therefore never to be too much of an exggeration to say that multi-turn conversation ability is the most significant part of LLMs.\nParrot: enhancing multi-turn instruction following for LLMs Multi-turn example, contextual information is need to be ultilized by LLMs. (Image source: Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models)\nThe most common interaction way between human and LLMs is multi-turn conversation. Parrot1 presents a solution aiming to enhancing multi-turn instruction following for LLMs.\nDataset Collection: the authors proposes training a specialized Parrot-Ask to generate queries using the available real user-ChatGPT logs based on LLaMA, then employ Parrot-Ask to interact with an assistant LLM and thus collect 40K multi-turn instruction tuning data. Training Parrot-Ask Model: Training the mode is the inverse of standard instruction tuning. Compare to common supervised finetuning methods, the Parrot-Ask model is trained to predict query tokens instead of assistant output tokens. Concretely, the authors use LLaMA-13B-Chat and 90K ShareGPT data to train this model. CaPO dataset Collection: The authors sample 10K dataset which involve contextual information and adapt three strageties to generate negative responses, thus collect 10K Context-Aware Preference Optimazation (CaPO) dataset. The process of Parrot.(a) First, train the Parrot-Ask model on real user-ChatGPT logs to learn how real users pose queries, and utilize it to iteratively interact with ChatGPT to collect multi-turn instructionresponse pairs. (b) Then construct negative responses for queries that rely heavily on context for answering with three strategies to simulate three types of error cases. Finally, use the collected data to train the Parrot-Chat model by (c) instruction tuning and (d) context-aware preference optimization.(Image source: Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models)\nhuman preference optimization Make LLMs refuse to answer unknown questions R-Tuning, introduced in (Zhang et al., 20232), aims to equip Large Language Models (LLMs) with the ability to decline answering unknown questions. It leverages the instruction tuning approach, following a two-step process:\nUncertainty Identification: The model is first evaluated on the training data. By inferring the model on the training data once and comparing the prediction and label, the instruction tuning data is split into uncertain data and certain data. Refusal-Aware Data Construction: Uncertainty expressions are appended to the labels of the certain data points. This newly constructed \u0026ldquo;refusal-aware data\u0026rdquo; is then used to fine-tune the LLM, enabling it to recognize and decline unknown questions. The workflow of constructing refusal-aware data. (Image source: R-Tuning: Teaching Large Language Models to Refuse Unknown Questions)\nThe purpose of R-Tuning is to alleviate hallucination of LLMs when facing unknown questions. However, it doesn\u0026rsquo;t take human preference responses into consideration.\ndirect preference optimization DPO (Direct Preference Optimization)3, which evolved from the pair-wise formulation of the reward model introduced in InstructGPT4, simplifies the RLHF (Reinforcement Learning from Human Feedback) process into a one-step optimization. The loss function has been reformulated as follows: $$\\mathcal{L}_{DPO}(\\pi_{\\theta};\\pi_{ref})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}[\\log\\sigma(\\beta\\log\\frac {\\pi_{\\theta}(y_w|x)} {\\pi_{ref}(y_w|x)} - \\beta\\log\\frac {\\pi_{\\theta}(y_l|x)} {\\pi_{ref}(y_l|x)} )]$$ where $y_w$ represents the accepted or preferred data, while $y_l$ represents the rejected or less preferred data. This formulation clearly demonstrates that DPO optimizes the margin between desirable and undesirable changes, effectively enhancing the model\u0026rsquo;s ability to generate preferred outputs.\nkto Sometimes, preference dataset with a one-pair format is hard to obtain. In such cases, we can use a set of preference data where each sample only has a label of \u0026lsquo;1\u0026rsquo; for accept or a label of \u0026lsquo;-1\u0026rsquo; for rejectable? KTO[^4] is proposed for this scenario.\nSun et al., Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models, 2024\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZhang et al., R-Tuning: Teaching Large Language Models to Refuse Unknown Questions, NAACL 2024\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRafailov et al., Direct Preference Optimization: Your Language Model is Secretly a Reward Model, 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOuyang et al., Training language models to follow instructions with human feedback, 2022\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/align-llms/","summary":"After pretraining on vast datasets and supervised fine-tuning with diverse instruction sets, Large Language Models (LLMs) have achieved remarkable capabilities in text generation. However, LLMs can generate seemingly reasonable sequences\u0026mdash;-free from grammatical errors and redundant words\u0026mdash;-they may still generate content that lacks truthfulness or accuracy. Are there any methods to mitigate these shortcomings? Researchers at OpenAI have framed these issues as the challenge of LLM alignment. Currently, one of the most prominent approaches to address these challenges is Reinforcement Learning from Human Feedback (RLHF).","title":"Align LLMs"},{"content":"audioLM Based on SoundStream1 and w2v-BERT2, audioLM3 proposes a framework which consist of three components: tokenizer model, decoder-only Transformer language model, detokenizer model. SoundStream, is neural audio codec with strong performance, which converts input waveforms at 16 kHZ into embeddings while w2v-BERT plays the role to compute semantic tokens. Figure 1. (Image source: AudioLM: a Language Modeling Approach to Audio Generation)\nFigure 2. The three stages of the hierarchical modeling of semantic and acoustic tokens in AudioLM: i) semantic modeling for long-term structural coherence, ii) coarse acoustic modeling conditioned on the semantic tokens and iii) fine acoustic modeling. With the default configuration, for every semantic token there are 2Q′ acoustic tokens in the second stage and 2(Q − Q′) tokens in the third stage. The factor of 2 comes from the fact that the sampling rate of SoundStream embeddings is twice as that of the w2v-BERT embeddings. (Image source: AudioLM: a Language Modeling Approach to Audio Generation)\nAudioLM is a pioneering framework that enables the generation of audio with a long-term coherent structure. It uniquely combines adversarial neural audio compression, self-supervised representation learning, and advanced language modeling techniques.\nZeghidour et al., SoundStream: An End-to-End Neural Audio Codec, 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChung et al., W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training, 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBorsos et al., AudioLM: a Language Modeling Approach to Audio Generation, 2022\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/audio-llms/","summary":"audioLM Based on SoundStream1 and w2v-BERT2, audioLM3 proposes a framework which consist of three components: tokenizer model, decoder-only Transformer language model, detokenizer model. SoundStream, is neural audio codec with strong performance, which converts input waveforms at 16 kHZ into embeddings while w2v-BERT plays the role to compute semantic tokens. Figure 1. (Image source: AudioLM: a Language Modeling Approach to Audio Generation)\nFigure 2. The three stages of the hierarchical modeling of semantic and acoustic tokens in AudioLM: i) semantic modeling for long-term structural coherence, ii) coarse acoustic modeling conditioned on the semantic tokens and iii) fine acoustic modeling.","title":"Audio LLMs"},{"content":"open-source component memo: A memory module of AI Agent for memorizing personal preferences, previous interactions, and business stages.\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/llm-agent/","summary":"open-source component memo: A memory module of AI Agent for memorizing personal preferences, previous interactions, and business stages.","title":"LLM Agent"},{"content":"Training an LLM needs a large amount of high qualitity data. Even though many giant teches open up their high performance LLMs (e.g., LLaMA, Mistral), high qualitity data still remain private.\nChinese Dataset English Dataset RefinedWeb: 600 B toknes Dolma: open-sourced by allenai, contains 3T tokens and a toolkit with some key features: high performance, portability, built-in tagger, fast decuplication, extensibility and cloud support. fineweb: 15 trillion tokens of high quality web data. Thanks to the team from huggingface. They filtered and deduplicated all CommonCrawl between 2013 and 2024. Models trained on FineWeb outperform RefinedWeb, C4, DolmaV1.6, The Pile and SlimPajama. fineweb-edu: curating through many methods from fineweb . Details you must want to know about are here: fineweb data processing technique report.\nDataComp: has 240T tokens from Common Crawl, and provides a collection of tool sets to undertake experiments in some ways (e.g., different mixing strategies, various data filters).\nclean dataset Huggingface introduces a python library datatrove to process, filter and deduplicate text data at a very large scale.\nSynthetic Data Generation instruction data generation Although some LLMs, like LLaMA, Gemma, have opened their weightings, their alignment data remain private, which obstructs the democratization of AI. The researchers find a way, named MAGPIE1, to generating large-scale alignment data. It is the key point that aligned LLMs can generate user query when we input only the left-side templates up to the right position reserved for user messages.\nFigure 1. The process of self-synthesizing instruction data from aligned LLMs (e.g., LLaMA-8B-Instruct) to create a high quality instruction dataset. (Image source: MAGPIE: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing)\nConcretely, the pipeline of MAGPIE is below:\nStep 1: Instruction Gneration: Magpie crafts an input query in the format of the predefined instruction template of the LLM. This query defines only the role of instruction provider (e.g., user), and does not provide any instruction. The auto-regressive LLM has been fine-tuned using instruction data in the format of the predefined instruction template. Thus, the LLM autonomously generates an instruction when the query crafted by Magpie is given as an input. Step 2: Response Generation: Magpie sends the instruction to the LLM to generate the responses. From the results of experiments, after fine-tuning LLaMA-8B-Base, the instruction dataset from MAGPIE outperforms other public datasets. ocr-free data generation MLLMs have been struggling in the task of text extraction from images. It is common to use OCR to extract text and then decode the key information in traditional ways, which requires much extra computation. It is now taken for granted that OCR-free methods will handle these tasks after MLLM (e.g., GPT4V) are developed. SynthDoG2 proposes a way for generating synthetic datasets at a large scale.\ndataset evaluation benchmark It is significantly crucial to find an approach to understand which data curation strategies work best and ultimately building better language model. DataComp-LM3(project) introduces a first benchmark for language model training data curation.\nFigure 2. The model get different results according to various training sets. A better training set can bring a better performance. (Image source: DataComp-LM: In search of the next generation of training sets for language models)\nIn this paper, the authors conduct 416 experiments with different training setsand compute scales, and identify that model-based filter play a key role in an effective data curation pipeline. Unbelievably, a simple bigram classifer, combined with a carefully selected set of positive and negative examples, performs best.\nFigure 3. The workflow of DCLM. (Image source: DataComp-LM: In search of the next generation of training sets for language models)\nDCLM, with a 200T token pool and 7B models, is the first large-scale data-centric benchmark for language models. Some interesting conclusions are blow:\namong C4, Dolam-V1, RedPajama, and RefinedWeb, RefinedWeb scores best. for text extraction (e.g., resiliparse, trafilatura(used by RefinedWeb), Common Crawl method), resiliparse and trafilatura have similar downstream perfomances, and resiliparse is even up to itimes faster. while decuplicating data, Bloom filter performs better than MinHash filter. Reference to model-based quality filtering, comparing to PageRank score filtering, Semantic Decuplication, linear classifiers fit on pre-trained BGE text embedding, AskLLM that prompts an LM to decide if a document is useful, Perplexity filtering, and Top-k average logits, fastText works best. Xu et al., MAGPIE: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing, 2024\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKim et al., OCR-free Document Understanding Transformer, ECCV 2022\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi et al., DataComp-LM: In search of the next generation of training sets for language models, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/data-manager-for-llms/","summary":"Training an LLM needs a large amount of high qualitity data. Even though many giant teches open up their high performance LLMs (e.g., LLaMA, Mistral), high qualitity data still remain private.\nChinese Dataset English Dataset RefinedWeb: 600 B toknes Dolma: open-sourced by allenai, contains 3T tokens and a toolkit with some key features: high performance, portability, built-in tagger, fast decuplication, extensibility and cloud support. fineweb: 15 trillion tokens of high quality web data.","title":"Data for LLMs"},{"content":"Large language models (LLMs) have already demonstrated significant achievements, many startups make a plan to train their own LLMs. However, training a LLM from scratch remains a big challenge, both in terms of machine costs and the difficulty of data collection. Under this background, continuous pretraining based on some open source LLMs is a considerable alternative.\nDetermine your purpose of your continuous pretraining LLM. In common, standard LLMs may not excel in specific domains like financial, law, or trade. And in these areas, the demands for LLMs are stringent. Given this, consistently training our own LLM is an advantageous decision. The three followings are what we\u0026rsquo;ve got to explore : 1) Is domain-adaptive continual pretraining helpful? 2) How can we adopt data selection strategy? 3) Whether the original capabilities are retained?\nData selection Data is the most essential component during continuous pretraining. Figure 1. All the metrics keep in line with data. The more the better. (Image source: 1st Multilingual Model Workshop - Continued Pre-training of LLMs​)\nHow do we select and combine various datasets? In important resampling (Xie et al1), the researchers introduce an approach known as Data Selection with Important Resampling (DSIR). The method utilizes raw and target data in an n-gram feature space to estimate important weights.\nFigure 1. For a raw dataset like The Pile, using an estimator to obtain importance weights, and then select data via the importance weights. (Image source: Data Selection for Language Models via Importance Resampling)\nTraining strategy Learning rate setting In general, we tend to use warming up strategy to fine-tune downstream models. But some researchers (Gupta et al.2) have drawn a series of interesting conclusions on warming up.\nDon\u0026rsquo;t use a maximum learning rate initially to avoid an initial large spike in the loss which leads to no consequence later. A smaller learning rate may preserve more performance on the upstream dataset. Continual pretraining with the latest pretrained checkpoint improves performance. Rewarming is not a good option when the downstream dataset is similar to the upstream dataset. For the same dataset, a constant learning rate achieves the best performance. Although a constant learning rate can give you a good initialization, rewarming will get better while training long enough. Catastrophic Forgetting What we commonly know about continual pretraining is catastrophic forgetting. Some standard solutions involve little more than mixing common data or retaining some gradient information, like EWA.\n(Li et al.3) finds continual pretraining may cause repetition issues.\nFigure 2. After continual training with the traditional Chinese dataset, the model starts to repeat a sentence. (Image source: Examining Forgetting in Continual Pre-training of Aligned Large Language Models)\nIt seems that catastrophic forgetting is an inevitable side effect when conducting continual pretraining. (Siriwardhana et al.)4 merge the trained mode with the original model using the TIES method, mitigating catastrophic forgetting.\nEvaluation tools lm-eval The lm-eval python package released by EleutherAI which aims to offer an open source framework in LLM evaluation for AI researchers. To install it, simply run a command:\npip install lm-eval Here is an interesting blog providing a tutorial for beginners.\nopencompass opencompass is a one-stop platform for large language model (LLM) evaluation.\nXie et al., Data Selection for Language Models via Importance Resampling, NIPS 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGupta et al., Continual Pre-Training of Large Language Models: How to (re)warm your model?, 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi et al., Examining Forgetting in Continual Pre-training of Aligned Large Language Models, 2024\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSiriwardhana et al., Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and Model Merging: A Comprehensive Evaluation, 2024\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/continuous-pretraining/","summary":"Large language models (LLMs) have already demonstrated significant achievements, many startups make a plan to train their own LLMs. However, training a LLM from scratch remains a big challenge, both in terms of machine costs and the difficulty of data collection. Under this background, continuous pretraining based on some open source LLMs is a considerable alternative.\nDetermine your purpose of your continuous pretraining LLM. In common, standard LLMs may not excel in specific domains like financial, law, or trade.","title":"Continual Pretraining"},{"content":"GLIDE ","permalink":"https://diqiuzhuanzhuan.github.io/posts/image-generation/","summary":"GLIDE ","title":"Image Generation"},{"content":"Recently, numerous AGI applications catch the eyes of almost all the people on the internet. Here lists some advanced papers elucidate their key principles and technologies.\nDiT The authors explore a new class of diffusion models based on the transformer architecture, Diffusion Transformers (DITs)1. Before their work, using a U-Net backbone to generate the target image is prevalent instead of using a transformer architecture. The authors make some experiments with variants of standard transformer blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens.\nFigure 1. DIT architecture. The gray area in the diagram is suboptimal structure. (Image source: Scalable Diffusion Models with Transformers)\nIn general, DiTs mainly introduce three components, patchily, DiT blocks, Decoder. All the diffusion process undergoes under the latent space projected by a VAE encoder. DiTs is based on Visual Transformer architecture (ViT) which operates on sequences of patches.\nFigure 2. Given the input of latent space, DiTs patchify it into a sequence. The length of sequence is relevant to patch size p. (Image source: Scalable Diffusion Models with Transformers)\nAs for DiT block, considering that zero-initializing the final batch norm scale factor $\\gamma$ in each block accelerates large-scale training in supervised learning setting, and that zero-initializing the final convolutional layer in each block prior to any residual connection brings benifits in Diffusion U-Net, the authors design adaLN-Zero block, as illustrated in Figure 1. Then a standard linear decoder is applied to decode sequence tokens into many tensors and get predicted noise and covariance. DiTs is the first transformer-based backbone for diffusion models that outperforms prior U-Net models and has a promising future through scaling it to larger models and token counts.\nVDT VDT (lu et al.2) features transformer blocks with modularized temporal and spatial attention modules to utilize the rich spatial-temporal representation inheried in transformers and introduce a unified spatial-temporal mask modeling mechanism. Figure 3. Main components and pipeline in VDT. (Image source: VDT: General-purpose Video Diffusion Transformers via Mask Modeling)\nTo avoid heavy computation, VDT also preojects the video into a latent space with a pre-trained VAE tokenizer form latent diffusion model (LDM). Then, following the approach of ViT, VDT divides the latent feature representation into non-overlapping patch. Moreover, on account of fusing spatial and temporal information, the authors add position embeddings and time embeddings. The key to integrating time information into transformer blocks is adding time component after the layer normalization. Similarly, incorporating condition feature frames involves adding these features into the layer normalization of transformer blocks.\nFigure 4. Incorporating conditional frame features into the layer-normalization of transformer blocks to predict next frame. (Image source: VDT: General-purpose Video Diffusion Transformers via Mask Modeling)\nLatte In this work, the authors present a novel latent diffusion transformers (Latte3), which adopts a video Transformer as the backbone. Latte a pre-trained variational autoencoder to encode input video into features in latent space, where tokens are extracted from encoded features. Then a series of Transformer blocks are applied to encode these tokens. There are inherent disparities between spatial and temporal information and numerous tokens extracted from input videos, hence the authors design four Transformer-based model variants from the perspective of decomposing the spatial and temporal dimensions of input videos.\nFigure 4. Four model variants are designed to capture spatio-temporal information in videos.Each block depicted in light orange represents a Transformer block. The standard Transformer block is employed in (a) and (b). (Image source: Latte: Latent Diffusion Transformer for Video Generation)\nSuppose there is a video clip in the latent space $V_{L} \\in \\mathbb{R}^{F\\times{H}\\times{W}\\times{C}}$, here $F, H, W, C$ represent the number of frames, heights, widths, and channel of video frames in the latent space respectively. Then we translate $V_L$ into a sequence of tokens, denoted as $\\hat{z}\\in \\mathbb{R}^{n_f\\times{n_h}\\times{n_w}\\times{d}}$. For the input of our model, $z = \\hat{z} + p$, where p means the spatio-temporal position embedding. For the spatial Transformer block, the authors reshape $z$ into $z_s \\in \\mathbb{R}^{n_f\\times{t}\\times{d}}$ (here $t = n_h \\times{n_w}$), and then for the temporal Transformer block, the workers reshape $z_s$ into $z_t \\in \\mathbb{R}^{t\\times{n_f}\\times{d}}$ as the input.\nTo embed a video clip, the authors also explore two methods: Uniform frame patch embedding and Compression frame patch embedding. In the first method, $n_f$, $n_h$, $n_w$ correspond to $F$, $\\frac{H}{h}$, and $\\frac{W}{w}$ when non-overlapping image patches are extracted from every video frame. In the other method, $n_f$ is equivalent to $\\frac{F}{s}$ in contrast to non-overlapping uniform patch embedding. In short, \u0026lsquo;Compression\u0026rsquo;, means a few frames are compressed.\nFigure 5. (a) uniform frame patch embedding. (b) compression frame patch embedding. (Image source: Latte: Latent Diffusion Transformer for Video Generation)\nFigure 6. (Image source: Image source: Latte: Latent Diffusion Transformer for Video Generation)\nText2Video-Zero TextVideo-Zero(Khachatryan et al.4) is a totally training-free, does not require massive computation powers or dozens of GPUs.\nPeeples et al., Scalable Diffusion Models with Transformers, CVPR 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLu et al., VDT: General-purpose Video Diffusion Transformers via Mask Modeling, CVPR 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMa et al., Latte: Latent Diffusion Transformer for Video Generation, CVPR 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKhachatryan et al., Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators, 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/video-generation/","summary":"Recently, numerous AGI applications catch the eyes of almost all the people on the internet. Here lists some advanced papers elucidate their key principles and technologies.\nDiT The authors explore a new class of diffusion models based on the transformer architecture, Diffusion Transformers (DITs)1. Before their work, using a U-Net backbone to generate the target image is prevalent instead of using a transformer architecture. The authors make some experiments with variants of standard transformer blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens.","title":"Video Generation"},{"content":"Large Language Models (LLMs) have show great promise in various artificial intelligence applications. It is becoming a trend to train a Large Language Model. Nevertheless even for many senior AI engineers, training these complex models remain a significant challenge. Here lists a series of issues you may encounter in the future.\ntorch.distributed.barrier() stuck during training with multi gpus At first, you should try to set the environment variable \u0026lsquo;NCCL_P2P_DISABLE=1\u0026rsquo;. If it works out, the solution is probably to disable ACS of Pcie in BIOS. You may need to reference to the link.\nraise RuntimeError(\u0026ldquo;Ninja is required to load C++ extensions\u0026rdquo;) You need to make sure Ninja compile system has been installed.\nsudo apt install ninja-build subprocess.Called.ProcessError: Command \u0026lsquo;[\u0026lsquo;which\u0026rsquo;, \u0026lsquo;c++\u0026rsquo;]\u0026rsquo; required no-zero exit status 1. This error means each node has to install c++ compiler. So just check if it has been installed on every machine.\nConnection reset by peer in function _create_c10d_store in file rendezvous.py. The default TCP server is set to run on the process denoted as rank0. However, since many processes start at different time, the process on rank0 can\u0026rsquo;t ensure that the server has already started before all of other client processes start. To address this issue, we need to ensure that the server (rank0) starts first.\nAttributeError: \u0026lsquo;LlamaAttention\u0026rsquo; object has no attribute \u0026lsquo;rope_theta\u0026rsquo;. To solve this problem, update transformers to 4.34.0 or above (Link).\nHow to install and update cuda? Reference to this.\nimprove your efficiency of llm Resize your vocabulary equivalent to multiple of 8f\nWhat should we do if the version of cuda does not match with my torch? The best solution is that install different version of cuda first, and switch on your demand. How to do it? Just run this command and select your desired item.\nupdate-alternatives --config cuda what\u0026rsquo;s the meaning of these variables LOCAL_RANK: the ids of workers within a node. WORLD_SIZE: the number of total workers. RANK: in fact, it means WORLD_RANK, and defines the ids of all the workers in the wolrd (all nodes combined). If the WORLD_SIZE is four, the RANK can be 0,1,2 or 3.\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/problems-you-may-encounter-while-distributed-training/","summary":"Large Language Models (LLMs) have show great promise in various artificial intelligence applications. It is becoming a trend to train a Large Language Model. Nevertheless even for many senior AI engineers, training these complex models remain a significant challenge. Here lists a series of issues you may encounter in the future.\ntorch.distributed.barrier() stuck during training with multi gpus At first, you should try to set the environment variable \u0026lsquo;NCCL_P2P_DISABLE=1\u0026rsquo;. If it works out, the solution is probably to disable ACS of Pcie in BIOS.","title":"Problems you may encounter during distributed training"},{"content":"With the swift development of deep neural networks, a multitude of models handling diverse information modalities like text, speech, images, and videos have proliferated. Among AI researchers, it\u0026rsquo;s widely acknowledged that multimodality is the future of AI. Let\u0026rsquo;s explore the advancements in multimodality in recent years.\nTexts \u0026amp; Images CLIP CLIP (radford et al., 20211) thinks learning directly from raw text about images is promising alternative which leverage much a boarder source of supervision. Based on the consideration of computation budget and performance, the authors choose contrastive representation learning over directly predicting objectives. To train this model, we need a text encoder and an image encoder to get text and image representations, and then maximize the cosine similarity of them.\nFigure 1. Illustrate how to train and inference.(Image source: Learning Transferable Visual Models From Natural Language Supervision)\nPseudocode for the core of an implementation of CLIP (radford et al., 20211) is here:\nimport numpy as np # image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # joint multimodal embedding [n, d_e] I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # scaled pairwise cosine similarities [n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # symmetric loss function labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2 CLIP (radford et al., 20211) offers significant benefits for that task has relative little data given its zero-shot capability. The study underscores the substantial potential of pre-training techniques for multimodal applications. Therefore, shortly thereafter, numerous applications based on CLIP emerged.\nALIGN ALIGN (jia et al., 20212) leverages a noisy data of over one bilion image alt-text pairs to train a model, which only has a simple dual-encoder architecture, to align visual and language representation of image and text pairs use a contrastive loss.\nFigure 2. A summary of ALIGN methods, which doesn\u0026rsquo;t need curated data. (Images source: Learning Transferable Visual Models From Natural Language Supervision)\nThe authors only apply minimal frequency-based filtering on image and text, such as filtering alt-texts that are shared by more than 10 images, and filtering irrelevant content (e.g., \u0026ldquo;1980x1080\u0026rdquo;, \u0026ldquo;alt_img\u0026rdquo;), etc. While pre-training, the authors construct two losses: one for image-to-text classification, other one for text-to-image classification: $$L_{i2t} = -\\frac{1}{N} \\sum^N_i \\log \\frac{\\exp(x_i^T y_i/\\sigma)}{\\sum_{j=1}^N \\exp(x_i^T y_j/\\sigma)}$$ $$L_{t2i} = -\\frac{1}{N} \\sum^N_i \\log \\frac{\\exp(y_i^T x_i/\\sigma)}{\\sum_{j=1}^N \\exp(y_i^T x_j/\\sigma)}$$ where $x_i$ and $y_j$ are the normalized of image in the $i$-th pair and that of text in the $j$-th pair, respectively. $N$ is the batch size, and $\\sigma$ is the temperature to scale the logits. It is worth noting that the temperature parameter $\\sigma$ is not set manually but learned jointly together with other parameters.\nViLT ViLT (kim et al., 20213) is a simple architecture for a vision-and-language model as it commissions the transformer module to handle vision features in place of a seperate deep vision embedder. This architecture concentrates most of the computation on modality interaction other than feature extraction, thereby achieves competent performance on vision-and-language tasks without using region features or deep convolutional visual encoder.\nFigure 3. Four cagegories of vision-and-language model. CLIP belongs to (b), ViLT belongs to (d). (Image source: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision)\nBuilding on pre-training objectives, the authors use image text matching (ITM) and masked language modeling (MLM). ViLT is competent to competitors which are heavily equipped with convolutional visual embedding networks (e.g., Faster R-CNN and ResNets). Hence, the authors conclude that future work on VLP should focus more on the modality interactions inside the transformer module rather than engaging in an arms race that merely powers up unimodal embedders. Figure 4. ViLT model architecture. (Image source: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision)\nALBEF Most of previous methods choose to employ a transformer-based multimodal encoder to jointly model visual and language tokens. because the visual tokens and text tokens are unaligned, it is challenging for multimodal encoder to learn a optimal interaction. ALBEF (li et al., 20214) introduce a contrastive loss to align the text and image representation before modality fussion. To improve data efficiency from raw noisy data, the study proposes momentum distillation, a self-training method which learns the pseudo targets produced by the momentum model. Figure 5. Illustration of ALBEF. It consists of three encoders: image encoder, text encoder, and a multimodal encoder. (Image source: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation)\nThe pretrain process is relative complex, which includes three objectives: image-text contrastive learning (ITC) on the unimodal encoders, masked language modeling (MLM) learning and image-text matching (ITM) on the multimodal encoder. specifically, the authors improve ITM with online contrastive hard negative mining.\nImage-Text Contrastive learning (ITC): $$p_{m}^{i2t}(I) = \\frac{\\exp(s(I; T_{m})/\\tau)}{\\sum_{m=1}^{M} \\exp(s(I; T_{m})/\\tau)},p_{m}^{t2i}(I) = \\frac{\\exp(s(T; I_{m})/\\tau)}{\\sum_{m=1}^{M} \\exp(s(T; I_{m})/\\tau)}$$\n$$\\mathcal{L}_{\\text{itc}} = \\frac{1}{2} \\mathbb{E}_{(I; T) \\sim{D} } [ \\text{H}(y^{i2t} (I), p^{i2t} (I)) + \\text{H} (y^{t2i}(T), p^{t2i} (T) )]$$\nHere, $s(I;T)$ denotes the similarity of image $I$ and text $T$, $\\tau$ is a learnable temperature parameter, $y^{i2t}$ and $y^{t2i}$ denote the one-hot ground truth similarity, and $\\text{H}$ is the cross ent\tropy of two objects. The image-text contrastive loss is $\\mathcal{L}_{\\text{itc}}$.\nMasked Language modeling (MLM):\nMLM task utilizes both the image and the text to predict the masked words. The masking strategy is the same as BERT (devlin et al., 20185). MLM minimazes a cross-entropy loss:\n$$\\mathcal{L}_{mlm} = \\mathbb{E}_{(I, \\widehat{T}) \\sim D} H( y^{msk}, p^{msk} (I, \\widehat{T}))$$\nwhere $\\widehat{T}$ denotes a masked text, $p^{msk}(I, \\widehat{T})$ denotes the model\u0026rsquo;s probability for a masked token, $y^{msk}$ is a one-hot vocabulary distribution in which the group truth token\u0026rsquo;s value is 1.\nImage-Text Matching (ITM):\nITM predicts if a pair of text and image is matched or not matched. The CLS token of multimodal encoder is used the joint representation of the image-text pair. The ITM loss is:\n$$\\mathcal{L}_{itm} = \\mathbb{E}_{(I, T) \\sim D} H( y^{itm}, p^{itm} (I, T))$$\nwhere $y^{itm}$ is a 2-dimensional one-hot vector representing the ground truth label.\nFinally, the full objective loss is:\n$$\\mathcal{L} = \\mathcal{L}_{itc} + \\mathcal{L}_{mlm} + \\mathcal{L}_{itm}$$\nMomentum Distillation:\nAs one-hot labels for ITC and MLM penalize all negative predictions regardless of their correctness, to address this, the authors propose to learn from pseudo-targets generated by the momentum model. The momentum model is a continuously evolving teacher which consists of exponential-moving-average (EMA) of the unimodal and multimodal encoders. For ITC task, the momentum loss is:\n$$\\mathcal{L}_{itc}^{mod} = (1 - \\alpha) \\mathcal{L}_{itc} + \\frac{\\alpha}{2} \\mathbb{E}_{(I, T) \\sim D}[KL(q^{i2t}(I)||p^{i2t}(I)) + KL(q^{t2i}(T) || p^{t2i}(T))]$$\nwhere $q^{i2t}$ and $q^{t2i}$ are the pseudo targets generated by the momentum model.\nSimilarly, the MLM momentum loss is:\n$$\\mathcal{L}_{mlm}^{mod} = (1 - \\alpha) \\mathcal{L}_{mlm} + \\alpha\\mathbb{E}_{(I, \\widehat{T}) \\sim D}KL(q^{msk}(I, \\widehat{T})||p^{msk}(I, \\widehat{T}))$$ ALBEF propels the multimodality model to a new height, leading to the development of numerous related studies.\nVLMO VLMO(Bao et al., 20226) presents a unified Vision-Language Pretrained Model (VLMO) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. For the sake of encoding various modalities (images, text, and image-text pairs) within single Transformer block, VLMO introduces Mixture-of-Modality-Experts (MoME). V-FFN is designed for image-only data, and L-FFN is for text-only data. However, during training with iamge-text pair data, all modules (V-FFN, L-FFN and VL-FFN) are utilized. Figure 6. Overview of VLMO pre-training. (Image source: VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts)\nAnother noteworthy case is that self-attention module is frozen during training with only text data. Figure 7. Mixture of multi-experts. (Image source: VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts)\nBLIP Most existing frameworks of vision language model (VLP) only excel in either understanding-based tasks or generation-based tasks. BLIP (li et al., 20227) presents a new framework to transfer flexible to both vision-language understanding and generation tasks. Unlike previous works, BLIP argues the noisy web texts are suboptimal for vision-language learning and address this problem by proposing a novel method.\nBLIP introduces two important contributions: a new model architecture named multimodal mixture of encoder-decoder (MED), and a new data bootstrapping method captioning and filtering (CapFilt) which aims to learn from noisy image-text pairs.\nFigure 8. Pre-training model architecture and objectives of BLIP (same parameters have the same color. ITC task is trained without cross-attention. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\nFigure 9. BLIP learning framworkd, include model workflow and data workflow. The bootstrapped data will be used to pre-train the model. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\nFigure 10. Examples of captions generated by BLIP, green texts is accepted by filter, and red texts is rejected by filter. (Image source: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\nMAE Vary from past approaches, MAE(he et al., 20228) proposes an architecture that involves masking random patches of an input image and reconstructing the missing pixels. Masking a high proportion of input images, such as 75%, yields a nontrivial and meaningful self-supervised task. In MAE, encoder and decoder are not in symmetry. The decoder is designed to predict the pixel value of the masked patch. The authors also study a variant whose reconstruction target is the normalized pixel values of each masked patch, and find that improves the representation quality.\nFigure 11. The architecture consist of an encoder and decoder. (Image source: Masked Autoencoders Are Scalable Vision Learners)\nTokens vs Pixels: the studiers compare tokens and pixels as the reconstruction terms of decoder, and the experiments show that tokenization is not necessary for MAE.\nFigure 12. Tokens vs Pixels. Tokens don\u0026rsquo;t bring any benifits. (Image source: Masked Autoencoders Are Scalable Vision Learners)\nMonkey Monkey(li et al., 20239) focuses on enhancing large multimodal models (LMMs) for high-resolution input and detailed scene understanding. Compared to the approach of directly interpolating the ViT to increase input resolution, Monkey utilizes a novel module that divides high-resolution images into smaller patches by a sliding window method. Each patch is processed independently by a static visual encoder, enhanced with LoRA adjustments and a trainable visual resampler.\nFigure 13. Each patch is processed by independently by a static visual encoder, enhanced with LoRA adjustments and a trainable visual resampler。 All patches are processed through the shared static Vit encoder, such as Vit-BigG with 2b parameters. (Image source: Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models)\nMini-Gemini Cambrian-1 Cambrian-110 is a herd of multimodal LLMs (MLLMs) designed with a vision-contric approach. Previous multimodal models have primarily focused on language understanding, often neglecting the learning of visual representations. In this study, the authors using LLM and visual instruction tuning as an interface to evaluate various visual representations, offer a new insight into different methods (self-supervised, strongly supervised, or combination thereof). Cambrian thinks multimodal connector has three main designs: Resamplers, Q-Formers, and MLP Projectors, nevertheness all of them, more or lesss, have a few issues. For instance, MLP Porjector leads to a large growth of visual tokens with image resolution.\nLLaVA-OneVision LLaVA-OneVision11 is the first single model that can simultaneously push the performance boundaries of open LMMs in three import vison scenarios: single-image, multi-image, and video senarios. LLava-OneVision is constituted by three components:\nLLM: the authros select Qwen-2 as LLM Vision Encoder: the authors consider the SigLIP as the vision encoder. Projector: the researchers choose a 2-layer MLP to project image features into the word embedding space. Another aspect noteworthy is the contribution about high quality data collection and curation. In total, LLaVA-OneVision release three datasets, including Single-Image 3.2M, OneVision1.6M.\nllava-onevision-exp2.png dataset docmatix, open to public by huggingface, contains 1.27 million image-text pairs.\nReferences Radford et al., Learning Transferable Visual Models From Natural Language Supervision, CVPR 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJia et al., Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision, ICML 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKim et al., ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision, ICML 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi et al., Align before Fuse: Vision and Language Representation Learning with Momentum Distillation, CVPR 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDevlin et al., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBao et al., VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts, CVPR 2022\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi et al., BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, CVPR 2022\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHe et al., Masked Autoencoders Are Scalable Vision Learners, CVPR 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi et al., Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models, CVPR 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTong et al., Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs, 2024\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi et al., LLaVA-OneVision: Easy Visual Task Transfer, 2024\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/evolution-of-multimodality/","summary":"With the swift development of deep neural networks, a multitude of models handling diverse information modalities like text, speech, images, and videos have proliferated. Among AI researchers, it\u0026rsquo;s widely acknowledged that multimodality is the future of AI. Let\u0026rsquo;s explore the advancements in multimodality in recent years.\nTexts \u0026amp; Images CLIP CLIP (radford et al., 20211) thinks learning directly from raw text about images is promising alternative which leverage much a boarder source of supervision.","title":"Evolution of Multimodality"},{"content":"With the advancement of large language models (LLMs), the significance of the context length they can handle is increasingly apparent. Let\u0026rsquo;s take a look at the evolution of positional encoding over the years to enhance the context processing capability of LLMs.\nVanilla Positional Encoding Why does Transformer need positional encoding? Actually, Transformer contains no recurrence and no convolution. To help the model to ultilize the order of the sequence, Vanilla Transformer (vaswani et al., 20171) introduced the concept of positional encoding and adopted a simple yet effective approach, using sine and cosine functions to generate positional encodings. This method allows the model to effectively capture the positional information of words in the sequence without adding additional parameters.\n$$ PE(pos,2i) = sin(pos/10000^{2i/d_{model}}) $$ $$PE(pos,2i+1) = cos(pos/10000^{2i/d_{model}})$$ where $pos$ is the position and $i$ is the dimension, $d_{model}$ is the word embedding dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2π$ to $10000 · 2π$. The authors chose this function because they hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.\nOf course, the authors also make a comparision with learned positon embedding (gehring et al., 20172), and the two versions all produce nearly identical results. Nevertheless, the sinusoidal version may allow the model to extrapolate to sequence lengths not encountered during training. To understand the position encoding deeply, let\u0026rsquo;s visualize the position matrix with a certain length.\nimport numpy as np import matplotlib.pyplot as plt def get_position_encoding_by_attention_is_all_your_need(seq_len, d, n=10000): P = np.zeros((seq_len, d)) for k in range(seq_len): for i in np.arange(int(d/2)): denominator = np.power(n, 2*i/d) P[k, 2*i] = np.sin(k/denominator) P[k, 2*i+1] = np.cos(k/denominator) return P P = get_position_encoding_by_attention_is_all_your_need(seq_len=200, d=512, n=10000) figure = plt.matshow(P) plt.gcf().colorbar(figure) Figure 1. Visulize the position matrix when n=10000,d=512,seq_len=200. (Image source: generated from code above.)\nRelative Positional Encoding Other view: Positonal Encoding is Nothing but computational budget Almost all the people think postional encoding is vital for Transformer though, a team from IBM Research, Facebook CIFAR AI, .etc, get a radically different conclusion (kazemnejad et al., 2024)3, that is position encodings are not essential for decoder-only Transformers to generalize to longer sequences.\nReferences Vaswani et al., \u0026ldquo;Attention Is All Your Need\u0026rdquo;, AAAI 2017\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGehring et al., \u0026ldquo;Convolutional Sequence to Sequence Learning\u0026rdquo;, NIPS 2017\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKazemnejad et al., \u0026ldquo;The Impact of Positional Encoding on Length Generalization in Transformers\u0026rdquo;, NIPS 2023\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/position-encoding-in-transformers/","summary":"With the advancement of large language models (LLMs), the significance of the context length they can handle is increasingly apparent. Let\u0026rsquo;s take a look at the evolution of positional encoding over the years to enhance the context processing capability of LLMs.\nVanilla Positional Encoding Why does Transformer need positional encoding? Actually, Transformer contains no recurrence and no convolution. To help the model to ultilize the order of the sequence, Vanilla Transformer (vaswani et al.","title":"Positional Encoding in Transformer"},{"content":"What Is Generative Models? A generative model can be seen as a way to model the conditional probability of the observed $X$ given a target $y$ (e.g., given a target \u0026lsquo;dog\u0026rsquo;, generate a picture of the dog). Once trained, we can easily sample a stance of $X$. While training a generative model is significantly more challenging than a discriminative model (e.g., it is more difficult to generate an image of a dog than to identify a dog in a picture), it offers the ability to create entirely new data.\nLatent Variable Model for the data $x$ we observe, we imagine a latent variable $z$ and model them as a joint distribution $p(x, z)$. Therefore we have this form: $$p(x) = \\int p(x,z)dz$$ Apply bayes\u0026rsquo;s theorem, so: $$p(x) = \\frac{p(x,z)}{p(z|x)}$$ The log-likelihood of $p(x)$ is below: $$\\begin{align*} \\log p(x) \u0026amp;= \\log \\int p(x,z)dz \\\\ \u0026amp;= \\log \\int \\frac{p(x,z)q_{\\phi}(z|x)}{q_{\\phi}(z|x)}dz \\\\ \u0026amp;= \\log \\mathbb{E}_{q_{\\phi}(z|x)}\\frac{p(x,z)}{q_{\\phi}(z|x)} \\\\ \u0026amp;\\geq \\mathbb{E}_{q_{\\phi}(z|x)} \\log \\frac{p(x,z)}{q_{\\phi}(z|x)} \\\\ \\end{align*}$$From above, we derive the term $\\mathbb{E}_{q_{\\phi}(z|x)} \\log \\frac{p(x,z)}{q_{\\phi}(z|x)}$ called Evidence Lower Bound (ELBO), therefore maximizing the ELBO becomes a proxy objective with which to optimize a latent variable model.\nVariable Autoencoders (VAE) The purpose of the variable autoencoders is to maximize ELBO by optimizing for the best $q_{\\phi}(z|x)$ amongst a family of posterior distribution parameters by $\\phi$. $$\\begin{align*} \\mathbb{E}_{q_{\\phi}(z|x)} \\log \\frac{p(x,z)}{q_{\\phi}(z|x)} \u0026amp;= \\mathbb{E}_{q_{\\phi}(z|x)} \\log \\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi}(z|x)} \\\\ \u0026amp;= \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] + \\mathbb{E}_{q_{\\phi}(z|x)} [\\log \\frac{p(z)}{q_{\\phi}(z|x)}] \\\\ \u0026amp;= \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{\\text{KL}}(q_{\\phi}(z|x) \\parallel p(z)) \\end{align*}$$ In conclusion, we obtain a decoder term $\\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]$ and an encoder term $D_{\\text{KL}}(q_{\\phi}(z|x) \\parallel p(z))$. Our objective is to maximize the first term and minimize the second term. The encoder of VAE is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior is typically assumed to follow a standard multivariate Gaussian: $$q_{\\phi}(z|x) = \\mathcal{N}(z;\\mu_{\\phi}(x),\\sigma^{2}(x)\\text{I})$$ $$p(z) = \\mathcal{N}(z;0,\\text{I})$$ And then the objective function can be rewritten through Monte Carlo sampling as below (here $z^{(l)}$ is sampled from $q_{\\phi}(z|x)$ for every $x$ in the dataset: $$\\underset{\\phi, \\theta}{\\mathrm{arg\\ max}}\\ \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{\\text{KL}}(q_{\\phi}(z|x) \\parallel p(z)) \\ \\approx \\underset{\\phi, \\theta}{\\mathrm{arg\\ max}} \\sum_{l=1}^{L}\\log p_{\\theta}(x|z^{(l)})-D_{\\text{KL}}(q_{\\phi}(z|x) \\parallel p(z))$$ there has been still a problem that $z^{(l)}$ is sampled and unable to optimize throuth gradient descent. To solve this issue, reparameterization trick rewrite the random variable as a deterministic function of a noise variable. $$z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon\\ \\ where\\ \\ \\ \\ \\epsilon \\in \\mathcal{N}(\\epsilon; 0,\\text{I})$$ where $\\odot$ represents an element-wise product. Under this reparameterized version of $z$, gradients can be computed by optimizing $\\mu_{\\phi}$ and $\\sigma_\\phi$. After training, new data can be generated while sample a latent variable from $p(z)$ and feed it into the decoder of VAE. Furthermore, when a powerful semantic latent space is learned, latent vector can be edited or controled before being passed into the decode to generate the desired data.\nHierarchical Variational Autoencoders ","permalink":"https://diqiuzhuanzhuan.github.io/posts/know-about-diffusion-models/","summary":"What Is Generative Models? A generative model can be seen as a way to model the conditional probability of the observed $X$ given a target $y$ (e.g., given a target \u0026lsquo;dog\u0026rsquo;, generate a picture of the dog). Once trained, we can easily sample a stance of $X$. While training a generative model is significantly more challenging than a discriminative model (e.g., it is more difficult to generate an image of a dog than to identify a dog in a picture), it offers the ability to create entirely new data.","title":"Know about diffusion models"},{"content":"techniques have improved on not only text data but also computer vision recently. Here we focus on Visual Language Model (VLM) based on transformers. In the begining, some researchers try to extend BERT to process visual data and make a success. For example, visual-BERT and ViL-BERT achive strong performances on many visual tasks by training on two different objectives: 1) masked modeling task that aims to predict the missing part of a given input; and 2) a match task that aims to predict if the text and the image content are matched.\nLater, some contrastive tasks have been explored.\n","permalink":"https://diqiuzhuanzhuan.github.io/posts/visual-language-model/","summary":"techniques have improved on not only text data but also computer vision recently. Here we focus on Visual Language Model (VLM) based on transformers. In the begining, some researchers try to extend BERT to process visual data and make a success. For example, visual-BERT and ViL-BERT achive strong performances on many visual tasks by training on two different objectives: 1) masked modeling task that aims to predict the missing part of a given input; and 2) a match task that aims to predict if the text and the image content are matched.","title":"Visual LLMs"}]